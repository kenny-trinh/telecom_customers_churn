---
title: "Telecom Customer Churn Prediction Analysis"
author:
  - "Kenny Trinh"
  - "Sabin Pun"
  - "Sarp Koc"
date: "2025-01-10"
output:
  pdf_document:
    toc: true
    fig_caption: true
    number_sections: true
    highlight: pygments
header-includes:
  - \pagenumbering{gobble}
---

\newpage
\pagenumbering{arabic}

```{r Setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 6, fig.height = 2, fig.align = "center" )

# List of required packages
required_packages <- c(
  "readr",
  'dplyr',
  'tidyverse',
  'caret',
  'nnet',
  'pROC',
  'ggplot2',
  'corrplot',
  'e1071',
  'mgcv',
  'gridExtra',
  "tinytex",
  "stringr",
  "tidyr",
  "xfun"
)

# Function to check and install missing packages
install_if_missing <- function(packages) {
  missing_packages <- packages[!packages %in% installed.packages()[, "Package"]]
  if (length(missing_packages) > 0) {
    install.packages(missing_packages)
  }
}

# Install any missing packages
install_if_missing(required_packages)

# Load all packages
lapply(required_packages, library, character.only = TRUE)

# Define data paths
data_dir <- "data/"
raw_data_dir <- paste0(data_dir, "raw/")
cleaned_data_dir <- paste0(data_dir, "cleaned/")
output_dir <- "output/"

# Create data directories if they don't exist
dir.create(data_dir, showWarnings = FALSE)
dir.create(raw_data_dir, showWarnings = FALSE)
dir.create(cleaned_data_dir, showWarnings = FALSE)

# Create output directory if it doesn't exist
dir.create(output_dir, showWarnings = FALSE)
```

# A Multi-Model Machine Learning Approach

## Executive Summary

This analysis employs multiple machine learning approaches to understand customer behavior and predict churn in the telecommunications sector. By analyzing a comprehensive dataset of customer information, we provide actionable insights for improving customer retention and service optimization.

## Business Context: The Story of "TeleConnect"

Meet **TeleConnect**, a mid-sized telecommunications company striving to maintain its foothold in a fiercely competitive market. Over the past year, the company has faced growing challenges: increased customer churn, stagnant revenue growth, and difficulty in predicting which services drive customer satisfaction and retention.

### The Challenge

TeleConnect's problems are multifaceted:

-   **High Churn Rates**: Nearly 20% of their customer base leaves each year, especially those on month-to-month contracts.
-   **Price Sensitivity**: Customers complain about high monthly charges, but TeleConnect lacks clarity on whether price adjustments would help.
-   **Service Bundling**: While TeleConnect offers multiple services—Internet, Streaming, Security—adoption patterns remain unclear. Are these services increasing customer value, or are they simply an added cost?

TeleConnect’s leadership knows the stakes: acquiring a new customer costs significantly more than retaining an existing one. Yet, without data-driven insights, their current retention strategies feel like guesswork.

**Why Machine Learning?**

To address these challenges, TeleConnect has embraced a multi-model machine learning approach, leveraging advanced analytics to transform their operations. By applying complementary models tailored to different aspects of customer behavior, TeleConnect seeks to:

1.  **Predict Monthly Charges**: Understand the drivers of customer spending to design smarter pricing strategies and optimize service bundles.
2.  **Analyze Customer Tenure**: Uncover patterns in contract types, payment methods, and service adoption that influence the length of customer relationships.
3.  **Forecast Churn**: Identify at-risk customers proactively and implement targeted retention efforts.
4.  **Segment Customers by Risk**: Group customers into risk categories to prioritize retention strategies effectively.

**The Opportunity**

By integrating machine learning models into its decision-making processes, TeleConnect can:

- **Reduce churn** by predicting at-risk customers and proactively addressing their concerns.
- **Maximize revenue** through optimized pricing and smarter service bundling.
- **Improve retention** by understanding what drives tenure and customer satisfaction.

## Dataset Overview

The dataset encompasses comprehensive customer information including:

To categorize the variables in your dataset into **continuous**, **categorical**, **count**, and **binomial**, let’s analyze them step by step:

```{r DatasetOverview, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=FALSE}
library(readr)

# Show the cleaned data as a table
d.cleaned_telecom_customer_churn <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

#classify variables types
variable_types <- sapply(d.cleaned_telecom_customer_churn, function(x) {
  if (is.numeric(x) && length(unique(x)) > 20) {
    "Continuous"
  } else if (length(unique(x)) == 2) {
    "Binomial"
  } else if (!is.numeric(x) || length(unique(x)) <= 20) {
    "Categorical"
  } else if (is.numeric(x) && all(x == as.integer(x))) {
    "Count"
  } else {
    "Other"
  }
})

print(variable_types)

#creating count datatypes:
# 1. Create service count variable (including all services)
d.cleaned_telecom_customer_churn$services_count <- rowSums(
  dplyr::select(d.cleaned_telecom_customer_churn,
                PhoneService, MultipleLines, InternetService, OnlineSecurity,
                OnlineBackup, DeviceProtection, TechSupport, StreamingTV,
                StreamingMovies) != "No"
)
```

**1. Continuous Variables:**

-   `tenure` (Number of months the customer has been with the company)
-   `MonthlyCharges` (Monthly charge in dollars)
-   `TotalCharges` (Cumulative charges in dollars)

**2. Categorical Variables:**

-   `MultipleLines` (No phone service/Yes/No)
-   `InternetService` (DSL/Fiber optic/No)
-   `OnlineSecurity` (No internet service/Yes/No)
-   `OnlineBackup` (No internet service/Yes/No)
-   `DeviceProtection` (No internet service/Yes/No)
-   `TechSupport` (No internet service/Yes/No)
-   `StreamingTV` (No internet service/Yes/No)
-   `StreamingMovies` (No internet service/Yes/No)
-   `Contract` (Month-to-month/One year/Two year)
-   `PaymentMethod` (Electronic check/Mailed check/Bank transfer/Credit card)

**3. Binomial Variables:**

-   `gender` (Male/Female)
-   `SeniorCitizen` (0/1)
-   `Partner` (Yes/No)
-   `PhoneService` (Yes/No)
-   `PaperlessBilling` (Yes/No)
-   `Churn` (Yes/No - Target variable)

### Key Variables and Their Business Significance

1.  **Critical Predictors**
    -   Tenure: Indicates customer loyalty and relationship duration
    -   Contract Type: Reflects commitment level
    -   Monthly Charges: Represents service value and potential price sensitivity
    -   Internet Service: Core service offering affecting overall satisfaction
2.  **Service Usage Indicators**
    -   Multiple service subscriptions suggest higher customer engagement
    -   Security and support services indicate value-added service adoption
    -   Streaming services usage reflects modern consumption patterns
3.  **Financial Metrics**
    -   Average monthly charges: \$64.76
    -   Payment methods diversity indicates billing flexibility
    -   Relationship between charges and service subscriptions

\newpage

# Exploratory Data Analysis

The exploratory data analysis focuses on understanding customer churn, service adoption, and tenure patterns. This section includes visual and statistical analysis to identify trends and factors influencing churn and customer behavior.

**Data Overview:**

The dataset contains 7,043 customers with key features like:
- Demographics (Tenure, Services Count)
- Contract & Payment details (Contract Type, MonthlyCharges)
- Churn Status (Target Variable)

**Log-Transformation of Charges (MonthlyCharges & TotalCharges)**

```{r EDA-Histograms-LogTransform, echo=FALSE, results='show', fig.width = 8, fig.height = 5, warning=FALSE, cache=TRUE}
# Load required libraries if not already loaded
library(ggplot2)
library(gridExtra)
library(dplyr)
library(corrplot)

# Apply log transformation
d.cleaned_telecom_customer_churn <- d.cleaned_telecom_customer_churn %>%
  mutate(log_MonthlyCharges = log(MonthlyCharges + 1),
         log_TotalCharges = log(TotalCharges + 1))

# Histograms before and after log transformation
p1 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = MonthlyCharges)) + 
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  ggtitle("Before Log Transformation (MonthlyCharges)")

p2 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = log_MonthlyCharges)) + 
  geom_histogram(bins = 30, fill = "darkred", alpha = 0.7) +
  ggtitle("After Log Transformation (MonthlyCharges)")

p3 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = TotalCharges)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  ggtitle("Before Log Transformation (TotalCharges)")

p4 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = log_TotalCharges)) + 
  geom_histogram(bins = 30, fill = "red", alpha = 0.7) +
  ggtitle("After Log Transformation (TotalCharges)")

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

- TotalCharges and MonthlyCharges were right-skewed, requiring log-transformation.
- After transformation, the distributions became more normal, improving model assumptions

## Key Findings from EDA

```{r EDA-Boxplots, echo=FALSE, results='show', fig.width = 8, fig.height = 3, warning=FALSE, cache=TRUE}

# Churn distribution
churn_dist <- ggplot(d.cleaned_telecom_customer_churn, aes(x = Churn, fill = Churn)) +
  geom_bar() +
  geom_text(stat='count', aes(label=..count..), position=position_dodge(width=0.9), vjust=-0.5) +
  theme_minimal() +
  labs(title = "Distribution of Customer Churn", x = "Churn Status", y = "Number of Customers")

# Churn Rate by Contract Type:
contract_churn <- ggplot(d.cleaned_telecom_customer_churn, aes(x = Contract, fill = Churn)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Churn Rate by Contract Type", x = "Contract Type", y = "Proportion")



#Monthly Charages vs Tenure
charges_tenure <- ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure, y = MonthlyCharges, color = Churn)) +
  geom_smooth(method = "loess", se = TRUE) + 
  theme_minimal() +
  labs(title = "Monthly Charges vs Tenure (Smoothed)", x = "Tenure (months)", y = "Monthly Charges ($)")


# Arrange all plots in a grid: Bar plots on the left, Line plot on the right
grid.arrange(churn_dist, contract_churn, charges_tenure, 
             layout_matrix = rbind(c(1,2), 
                                   c(3,3)))

```

1. Churn Distribution
  - 26.5% of customers churned (1,869 out of 7,043).
  - Class imbalance exists (need to consider resampling for modeling).
2. Contract Type & Churn Relationship
  - Month-to-month contracts have the highest churn rate (42.7%).
  - Two-year contracts have the lowest churn rate (11.2%).
3.Relationship Between Monthly Charges & Tenure
  - Customers with lower tenure & high MonthlyCharges churn more frequently.
  - The relationship is non-linear, justifying using Generalized Additive Models(GAMs).
  
**Correlation Analysis with numeric variables **

```{r EDA-CorrelationMatrix, echo=FALSE, results='show', fig.width = 5, fig.height = 5, warning=FALSE, cache=TRUE}
# Selecting numeric variables
numeric_vars <- d.cleaned_telecom_customer_churn %>%
  select(tenure, log_MonthlyCharges, log_TotalCharges, services_count) %>%
  na.omit()

# Correlation matrix
corr_matrix <- cor(numeric_vars)

# Heatmap
corrplot(corr_matrix, method = "color", type = "upper")

```

**Correlation Analysis**

- TotalCharges and Tenure have a very high correlation (r $\approx$ 0.98), meaning one could be removed in some models to avoid multicollinearity.

Based on these findings, we proceed to the methodology section, where we implement various models to predict churn and evaluate their effectiveness.

# Methodology and Model Selection

Our analysis employs six complementary machine learning approaches, each chosen for specific analytical capabilities:

Our analysis employs a comprehensive suite of models, each chosen for specific analytical capabilities:

In our analysis, we employed the following machine learning approaches:

1.  **Linear Model (LM)**: To predict monthly charges and understand service impacts
2.  **GLM-Poisson**: To analyze customer tenure patterns and retention factors
3.  **GLM-Binomial**: To predict customer churn probability
4.  **Generalized Additive Model (GAM)**: To capture non-linear relationships in customer behavior
5.  **Neural Network (NN)**: To recognize complex patterns in customer churn
6.  **Support Vector Machine (SVM)**: To segment customers based on churn risk and grouping them.

\newpage

# Linear Model (LM)

*Sabin Pun took the lead on the linear model.*

To address **TeleConnect’s price sensitivity challenge**, we began by building a **Linear Model (LM)** to understand the key drivers of MonthlyCharges, we first build a Linear Model (LM) as a baseline approach. This helps TeleConnect analyze the impact of various customer attributes on pricing and service bundling

The response variable for this linear model is:

- **MonthlyCharges**: The amount customers pay per month(continuous)

The predictors used are:
1.  **Tenure**: Number of months the customer has been with the company.
2.  **Contract:** Type of contract( Month to Month, one year, two year)
3.  **InternetService**: Type of internet service (DSL, Fiber optic, No internet service).
4.  **StreamingTV**: Whether customers have streaming TV services (Yes/No).
5.  **PhoneService**: Whether customers have a phone service (Yes/No).
6.  **MultipleLines**: Whether customers have multiple phone lines (Yes/No).

## Linear Model to predict monthly charges

```{r LM-Model, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Load required libraries
library(ggplot2)

# Initial linear model with all relevant predictors
lm_initial <- lm(log(MonthlyCharges) ~ 
                   tenure + 
                   InternetService + 
                   Contract + 
                   StreamingTV + 
                   StreamingMovies +
                   OnlineSecurity +
                   OnlineBackup +
                   DeviceProtection +
                   TechSupport +
                   PhoneService,
                 data = d.cleaned_telecom_customer_churn)

# Get model summary
summary(lm_initial)
```

**Coefficient Interpretation**:

| **Predictor** | **Effect on Log(Monthly Charges)** |
| --- | --- |
| **Tenure** | `+0.00064` per month |
| **Internet Service (Fiber Optic)** | `+0.347` (**+41.5%**) |
| **Internet Service (No Service)** | `-0.863` (**-57.8%**) |
| **Contract (Two Years)** | `+0.00882` (**+9.2%**) |
| **Additional Services** | **Increase charges by 6% - 55%** |
 
: Coefficient interpretation for the linear model

**Interpretation:**

- **Tenure**: Longer tenure slightly increases monthly charges.
- **Internet Service**: Fiber optic users pay 41.5% more than DSL users, while customers with no internet service pay 57.8% less.
- **Contract Type**: Two-year contracts increase monthly charges by 9.2%.
- **Additional Services (includes Streaming, Security, Backup, Tech Support, Device Protection and Phone Service)**: Customers with multiple services pay 6% to 55% more, with Phone Service having the most significant impact.


## Visualizing the main effects

**Monthly Charges against Tenure:**/ 
**Monthly Charges vs Tenure:**

```{r LM-Visualizations, echo=FALSE, results='show', cache=TRUE}
# Plot 1: Log Monthly Charges vs Tenure by Internet Service
plot1 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure, y = log(MonthlyCharges), color = InternetService)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "loess", se = TRUE) + 
  theme_minimal() +
  labs(title = "Log Monthly Charges vs Tenure by Internet Service",
       x = "Tenure (months)", y = "Log Monthly Charges")

# Plot 2: Log Monthly Charges vs Tenure by Contract Type
plot2 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure, y = log(MonthlyCharges), color = Contract)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "loess", se = TRUE) + 
  theme_minimal() +
  labs(title = "Log Monthly Charges vs Tenure by Contract Type",
       x = "Tenure (months)", y = "Log Monthly Charges")

# Arrange plots in grid view
grid.arrange(plot1, plot2, nrow = 2)
```

## Limitations of the Linear Model

While the linear model provides useful insights, it assumes a strictly linear relationship between features and Monthly Charges. However, as observed, some results—such as contract type pricing—contradict expectations.The visualizations indicate non-linear relationships between Tenure and Log Monthly Charges. This suggests that contract pricing follows a non-linear trend, which cannot be captured effectively by a simple linear model

- Contract types & internet services exhibit distinct trends, suggesting interactions that LM cannot fully capture.
- LOESS smoothers in the plots show curved trends, implying that a Generalized Additive Model (GAM) or non-linear methods (SVM, Neural Networks) might be better suited.

Therefore, while LM helps interpret key relationships, we move forward with GLMs, GAMs, and advanced models to improve predictive accuracy.

\newpage

# GLM-Poisson Model
*Sabin Pun took the lead on the GLM-Poisson model.*

To address **TeleConnect’s challenge of understanding customer retention**, we utilized a **GLM-Poisson model** to analyze the factors influencing **tenure**—the number of months a customer remains subscribed—is a key indicator of retention. A longer tenure generally means higher customer lifetime value (CLV), while shorter tenures indicate early churn. 

To analyze customer tenure, we use a GLM-Poisson regression model, which is well-suited for count data like tenure (measured in months).

**Tenure** (count of months as a customer).

Include relevant predictors, such as:

-   `Contract`: Longer contracts may correlate with higher tenure.
-   `InternetService`: Different internet services might drive customer loyalty.
-   `PaymentMethod`: Certain payment methods might affect tenure (e.g., automatic payments may reduce churn).
-   `StreamingTV` and `StreamingMovies`: Value-added services could contribute to customer retention.
-   `SeniorCitizen`, `Partner`, `Dependents`: Demographics that might influence tenure.

**Exploring the Data:**

```{r Poisson-DataExploration, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Load required libraries
library(ggplot2)

ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Tenure",
       x = "Tenure (Months)",
       y = "Frequency") +
  theme_minimal()
```

-   There’s a high frequency of customers with **tenure close to 0 months**.
-   There’s another peak at **tenure near 72 months**, likely customers who have been with the company long-term.


## Visualization.

**Observed vs Predicted Tenure**

```{r Poisson-Visualizations, echo=FALSE, results='show', cache=TRUE}
# Fit Poisson regression
glm_poisson <- glm(formula = tenure ~ InternetService + Contract + PaymentMethod + 
                     StreamingTV + StreamingMovies + SeniorCitizen + Partner + 
                     log(MonthlyCharges), 
                   family = poisson(link = "log"),
                   data = d.cleaned_telecom_customer_churn)

# Predict tenure using the Poisson model
d.cleaned_telecom_customer_churn$predicted_tenure <- predict(glm_poisson, type = "response")

ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure, y = predicted_tenure)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red") +
  theme_minimal() +
  labs(title = "Observed vs. Predicted Tenure",
       x = "Observed Tenure (Months)",
       y = "Predicted Tenure (Poisson Model)")
```

**Observed vs. Predicted Tenure (Poisson Model)**

  - The model underestimates shorter tenures and overestimates longer tenures, indicating   potential overdispersion or non-linearity.
  - Suggests that Poisson may not be the best fit, reinforcing the need for Quasi-Poisson     or Negative Binomial.

## Overdispersion Check**

To ensure that the **Poisson model** is appropriate for modeling **customer tenure**, we checked for **overdispersion**. Overdispersion occurs when the variance of the response variable is significantly greater than its mean, which can lead to underestimated standard errors and misleading statistical inferences.

### **Dispersion Parameter Calculation**
The dispersion parameter is calculated as:

\[
\text{Dispersion} = \frac{\text{Residual Deviance}}{\text{Degrees of Freedom}}
\]

For our **Poisson model**, we obtained:

- **Residual Deviance**: `74,160`
- **Degrees of Freedom**: `7,030`
- **Dispersion Parameter**: `10.55`

**Interpretation**
- Since the dispersion parameter **(10.55)** is **significantly greater than 1**, this indicates **strong overdispersion**.
- This suggests that the **Poisson model underestimates variability**, making it **not the best fit** for the data.

### **Solution: Quasi-Poisson Model**
To address overdispersion, we proceed with a **Quasi-Poisson model**, which adjusts the standard errors to provide more reliable estimates.


## Quasi-Poisson Model

Since the **Poisson model showed significant overdispersion**, we use a **Quasi-Poisson model** to correct for underestimated standard errors. The **Quasi-Poisson model** allows for variability greater than the mean, providing more reliable coefficient estimates.

```{r Quasi-Poisson-Model, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Fit Quasi-Poisson Model
glm_quasi <- glm(tenure ~ InternetService + Contract + PaymentMethod + 
                   StreamingTV + StreamingMovies + SeniorCitizen + Partner + 
                   log(MonthlyCharges), 
                 family = quasipoisson(link = "log"), 
                 data = d.cleaned_telecom_customer_churn)

# Display model summary
summary(glm_quasi)

```


## Quasi-Poisson Model Interpretation

| **Predictor** | **Effect on Log(Tenure)** | **Interpretation** |
| --- | --- | --- |
| **Internet Service (Fiber Optic)** | `-0.124` (**↓11.6% tenure**) |
| **Internet Service (No Service)** | `+0.509` (**↑66.4% tenure**) |
| **Contract (One Year)** | `+0.731` (**↑107.7% tenure**) | 
| **Contract (Two Year)** | `+0.970` (**↑164.4% tenure**) | 
| **Payment Method (Credit Card - Automatic)** | `
| **Payment Method (Electronic Check)** | `-0.157` (**↓14.5% tenure**) | 
| **Payment Method (Mailed Check)** | `-0.362` (**↓30.4% tenure**) | 
| **Senior Citizen** | `+0.118` (**↑12.5% tenure**) |
| **Partner (Yes)** | `+0.236` (**↑26.6% tenure**) | 
| **Log(Monthly Charges)** | `+0.615` (**↑84.9% tenure per unit increase**) | 

: **Coefficient for the Quasi-Poisson model**.

---
**Interpretation**
-  Customers with **Fiber optic internet** have **11.6% shorter tenure** compared to DSL users. 
- Customers with **no internet service** have **66.4% longer tenure** than DSL users.
- Customers with a **one-year contract stay 107.7% longer** than those on **month-to-month contracts**
- Customers with a **two-year contract stay 164.4% longer**, highlighting **strong retention** for long-term contracts.
- Customers paying via **electronic check** have **14.5% shorter tenure**, suggesting potential churn risks. |
- Customers using **mailed checks** have **30.4% shorter tenure**, indicating **high churn risk**. 
-  Senior citizens tend to stay **12.5% longer** than younger customers.
-  Customers **with partners** stay **26.6% longer**, indicating stability in tenure. 
- **Higher monthly charges** are associated with **longer tenure**, possibly due to bundled services. |




### **Key Takeaways**
- **Contract type remains the strongest predictor** of customer retention.
- **Higher-paying customers** tend to stay longer.
- **Payment method influences tenure**: Customers using **mailed checks churn much faster**.
- **Internet service impacts tenure in unexpected ways**: Customers without internet **stay longer**.





\newpage

# GLM-Binomial (Logistic Regression) & GAM
*Sarp Koc took the lead on the GLM-Binomial and GAM models.*

## Context and Objective

The GLM-Binomial (Logistic Regression) and Generalized Additive Model (GAM) are used to predict customer churn and explore its underlying drivers. These models provide a foundation for identifying at-risk customers and evaluating key variables such as tenure, contract type, and payment methods. The analysis focuses on determining whether non-linear relationships exist in the data and comparing the performance of GLM and GAM in capturing these dynamics. By leveraging both linear and non-linear approaches, this section aims to enhance TeleConnect's retention strategies through actionable insights.

## Data Preparation and Visualization

First let's prepare the data for the visualization and modelling.

```{r GLM-Binomial-GAM-DataPreparation, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}

# Load required libraries
library(ggplot2)
library(dplyr)
library(mgcv)
library(pROC)
library(caret)

# Load the data
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)
data$Churn <- ifelse(data$Churn == "Yes", 1, 0)
# Ensure variables are factors
data$Contract <- as.factor(data$Contract)
data$PaymentMethod <- as.factor(data$PaymentMethod)
data$Churn <- as.factor(data$Churn)
data$TotalCharges <- log(data$TotalCharges)
data$MonthlyCharges <- log(data$MonthlyCharges)
data$InternetService <- as.factor(data$InternetService)

#str(data)
#colSums(is.na(data))
```

Let's plot some results to understand the data better.

## Plot: Tenure vs. TotalCharges by Churn

```{r GLM-Binomial-GAM-Visualizations, echo=FALSE, results='show', cache=TRUE}
# Ensure necessary libraries are installed
if(!require(ggplot2)) install.packages("ggplot2")

# Load the library
library(ggplot2)


ggplot(data, aes(x = tenure, y = TotalCharges, color = Churn)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Relationship Between Tenure and Total Charges by Churn",
    x = "Tenure (Months)",
    y = "Total Charges"
  ) +
  scale_color_manual(values = c("blue", "red"), labels = c("No Churn", "Churn")) +
  theme_minimal()
```

Higher tenure and total charges are associated with lower churn, reinforcing loyalty among long-term customers with higher spending.

## Plot: Tenure by Contract Type

```{r GLM-Binomial-GAM-Visualizations2, echo=FALSE, results='asis', cache=TRUE}

ggplot(data, aes(x = Contract, y = tenure, fill = Contract)) +
  geom_boxplot(alpha = 0.7) +
  labs(
    title = "Tenure by Contract Type",
    x = "Contract Type",
    y = "Tenure (Months)"
  ) +
  scale_fill_manual(values = c("blue", "orange", "purple")) +
  theme_minimal()
```

Longer contract durations (e.g., two years) correlate with higher customer retention, likely due to lower churn incentives.

## Plot: Churn by Payment Method

```{r GLM-Binomial-GAM-Visualizations3, echo=FALSE, results='asis', cache=TRUE}

ggplot(data, aes(x = PaymentMethod, fill = Churn)) +
  geom_bar(position = "fill", alpha = 0.8) +
  labs(
    title = "Proportion of Churn by Payment Method",
    x = "Payment Method",
    y = "Proportion"
  ) +
  scale_fill_manual(values = c("blue", "red"), labels = c("No Churn", "Churn")) +
  theme_minimal() +
  coord_flip()
```

Customers using electronic checks exhibit the highest churn proportion, suggesting potential dissatisfaction or difficulties with this payment method.

## Plot: Churn Proportion by Internet Service

```{r GLM-Binomial-GAM-Visualizations4, echo=FALSE, results='asis', cache=TRUE}
# Load necessary libraries
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(dplyr)) install.packages("dplyr")

library(ggplot2)
library(dplyr)

# Create a dataset for proportions and counts
proportion_df <- data %>%
  group_by(InternetService, Churn) %>%
  summarise(Count = n()) %>%
  mutate(Total = sum(Count),
         Proportion = Count / Total)

# Plot the bar chart with counts displayed
ggplot(proportion_df, aes(x = InternetService, y = Proportion, fill = as.factor(Churn))) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = Count), position = position_dodge(width = 0.9), vjust = -0.2, size = 3) +
  labs(
    title = "Proportion of Churn by Internet Service with Counts",
    x = "Internet Service Type",
    y = "Proportion",
    fill = "Churn"
  ) +
  scale_fill_manual(values = c("blue", "red"), labels = c("No", "Yes")) +
  theme_minimal()
```

Fiber optic users have significantly higher churn rates than DSL or users without internet service, indicating dissatisfaction with fiber optic services.

Let's create models of GLM and GAM to see if they catch these relations and how will they perform compared to each other.

## GLM

```{r GLM, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}

# Set 'No service' as the reference level for InternetService
data$InternetService <- relevel(data$InternetService, ref = "No")
# Confirm reference level
cat("Reference level for 'Contract':", levels(data$Contract)[1], "\n")
cat("Reference level for 'InternetService':", levels(data$InternetService)[1], "\n")
# Fit the GLM with interaction terms
glm_model_interaction <- glm(Churn ~ SeniorCitizen + Contract + PaymentMethod + 
                             tenure + TotalCharges + 
                             InternetService + OnlineSecurity + TechSupport + 
                             PaperlessBilling + MultipleLines, 
                             family = binomial, data = data)
# Check the summary
summary(glm_model_interaction)

```

So let's interpret the results:

Fiber optic users are more likely to churn, while DSL users are less likely to churn compared to no internet service customers.

Electronic check users are at higher risk of churn.

Customers with higher tenure and total charges are less likely to churn.

Longer contracts significantly reduce churn risk.

Senior citizens are more likely to churn

Electronic check users are more likely to churn.

Online security increases churn

Tech support significantly increases churn risk.

Longer contracts significantly reduce churn.

## Did the model overfit?

```{r GLM-Overfitting, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(pROC)
# Split data into training and test sets
set.seed(123)
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

# Predict on training and test data
train_preds <- predict(glm_model_interaction, train_data, type = "response")
test_preds <- predict(glm_model_interaction, test_data, type = "response")

# Calculate performance metrics
library(pROC)
train_auc <- roc(train_data$Churn, train_preds)$auc
test_auc <- roc(test_data$Churn, test_preds)$auc

cat("Train AUC:", train_auc, "\n")
cat("Test AUC:", test_auc, "\n")
```

The Train AUC (0.847) and Test AUC (0.854) are very close, indicating that the model performs consistently on both the training and test datasets. This suggests good generalization without overfitting to the training data.

```{r GLM-CrossValidation, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(caret)

# Set up cross-validation
control <- trainControl(method = "cv", number = 10) # 10-fold cross-validation

# Train the model
cv_model <- caret::train(Churn ~ SeniorCitizen + Contract + PaymentMethod + 
                  tenure + MonthlyCharges + TotalCharges + 
                  InternetService + InternetService:MonthlyCharges, 
                  data = data, method = "glm", family = "binomial", trControl = control)

# Check cross-validation results
print(cv_model)
```

If there were significant overfitting, we would expect a much larger difference between training and cross-validation accuracy. So it seems there is no overfit

## GAM with non-linear patterns

```{r GAM-WithNonLinearPatterns, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(mgcv)
data$TotalCharges_centered <- data$TotalCharges- mean(data$TotalCharges)
cat("Reference level for 'Contract':", levels(data$Contract)[1], "\n")
cat("Reference level for 'InternetService':", levels(data$InternetService)[1], "\n")
gam_smoothing_spline <- gam(Churn ~ SeniorCitizen + Contract + PaymentMethod + 
                             s(tenure) + s(TotalCharges_centered) + 
                             InternetService + OnlineSecurity + TechSupport + 
                             PaperlessBilling + MultipleLines, 
                             family = binomial, data = data, select=TRUE)
summary(gam_smoothing_spline)
```

edf shows that mooth terms for tenure and TotalCharges_centered are highly significant (p \< 0.001), indicating non-linear effects. Relationship is non-linear but not overly complex.

## ROC Comparison

Let's compare these models.

```{r ROC-Comparison, echo=FALSE, results='asis', cache=TRUE}

library(pROC)

# Predictions for glm_model
pred1 <- predict(glm_model_interaction, type = "response")
roc1 <- roc(data$Churn, pred1)

pred2 <- predict(gam_smoothing_spline, type = "response")
roc2 <- roc(data$Churn, pred2)

# Plot ROC curves
plot(roc1, col = "blue", lwd = 2, main = "ROC Curve Comparison")
lines(roc2, col = "red", lwd = 2)
legend("bottomright", legend = c("glm_model_interaction", "gam_smoothing_spline"),
       col = c("blue", "red"), lwd = 2)

auc1 <- auc(roc1)
auc2 <- auc(roc2)
cat("AUC of Model glm_model_interaction:", auc1, "\n")
cat("AUC of Model gam_smoothing_spline:", auc2, "\n")
```

Both models perform similarly in terms of AUC, but GAM slightly edges out GLM in predictive performance.

```{r Anova-Comparison, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
anova(glm_model_interaction, gam_smoothing_spline, test = "Chisq")
```

GAM explains more variance than GLM.

```{r AIC-Comparison, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
AIC(glm_model_interaction, gam_smoothing_spline)
```

GAM has a slightly lower AIC (5809.896) than GLM (5828.536), further supporting that GAM provides a better fit to the data.

## Conclusion

Both models performed well in predicting churn, with GAM offering additional flexibility to capture non-linear relationships. While the non-linearity in some variables, like tenure, was not significant, GAM successfully identified a meaningful non-linear relationship with TotalCharges. The results highlight that customers are more likely to churn if they use fiber optic internet, pay via electronic checks, lack online security or tech support, or are senior citizens. Conversely, churn risk is lower for DSL users, those with higher tenure and total charges, and customers on longer-term contracts.

# Neural Network (NN)
*Kenny Trinh took the lead on the Neural Network model.*

## Context and Objective

TeleConnect aims to leverage Neural Network (NN) models to enhance customer retention by identifying churn-prone customers with high accuracy. The NN model was trained and tested to predict churn outcomes using customer behavioral and demographic data. This analysis focuses on understanding key metrics like sensitivity, specificity, and precision to refine TeleConnect's retention strategies.

## Data Preparation for Neural Network

```{r NN-DataPreparation, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Load necessary libraries
library(caret)
library(nnet)
library(pROC)
library(ggplot2)
library(dplyr)

# Load the cleaned dataset
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

# Convert categorical variables to factors
data$Churn <- as.factor(ifelse(data$Churn == "Yes", 1, 0))

# Check and convert other categorical variables
data$gender <- as.factor(data$gender)
data$Partner <- as.factor(data$Partner)
data$PhoneService <- as.factor(data$PhoneService)
data$MultipleLines <- as.factor(data$MultipleLines)
data$InternetService <- as.factor(data$InternetService)
data$OnlineSecurity <- as.factor(data$OnlineSecurity)
data$OnlineBackup <- as.factor(data$OnlineBackup)
data$DeviceProtection <- as.factor(data$DeviceProtection)
data$TechSupport <- as.factor(data$TechSupport)
data$StreamingTV <- as.factor(data$StreamingTV)
data$StreamingMovies <- as.factor(data$StreamingMovies)
data$Contract <- as.factor(data$Contract)
data$PaperlessBilling <- as.factor(data$PaperlessBilling)
data$PaymentMethod <- as.factor(data$PaymentMethod)

# Convert TotalCharges to numeric (handle any non-numeric issues)
data$TotalCharges <- as.numeric(as.character(data$TotalCharges))

data <- na.omit(data)  # Remove rows with missing values

# Split the data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(data$Churn, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Preprocess the data: scale numeric columns
preProc <- preProcess(trainData[, c("tenure", "MonthlyCharges",
                                    "TotalCharges")],
                      method = c("center", "scale"))
trainData[, c("tenure", "MonthlyCharges", "TotalCharges")] <- predict(
  preProc, trainData[, c("tenure", "MonthlyCharges", "TotalCharges")])
testData[, c("tenure", "MonthlyCharges", "TotalCharges")] <- predict(
  preProc, testData[, c("tenure", "MonthlyCharges", "TotalCharges")])
```

The data is prepared for the Neural Network model by splitting it into training and testing sets and scaling numerical features for consistent input ranges. This step ensures the NN model can effectively learn patterns.



## Training the Neural Network

### Exploring different Neural Network Configurations

| **Config.** | **Size** | **Decay** | **Accuracy** | **Sensitivity** | **Specificity** | **Balanced Accuracy** | **Kappa** |
|--------|--------|--------|--------|--------|--------|--------|--------|
| **1** | **2** | **0.04** | **81.38%** | **91.10%** | **54.42%** | **72.76%** | **0.4879** |
| **2** | 1 | 0.04 | 80.88% | 90.04% | 55.50% | 72.77% | 0.4813 |
| **3** | 7 | 0.04 | 79.74% | 88.30% | 56.03% | 72.17% | 0.4603 |
| **4** | 5 | 0.04 | 80.60% | 90.14% | 54.16% | 72.15% | 0.4707 |

: Neural Network Configurations and Performance Metrics

**Comparison:**

- Configuration 1:
  - Best overall.
  - Has the best balance across all metrics.
- Configuration 2:
  - High simplicity.
  - Simple with strong performance.
- Configuration 3:
  - Best trade-off.
  - Balanced with slightly higher specificity.
- Configuration 4:
  - Moderate complexity.
  - Great balance.
  
**Key Insights from Neural Network Configurations:**

1.  **Best Overall Model**:
    -   **Size = 2, Decay = 0.04** delivers the best trade-off between sensitivity (91.10%) and specificity (54.42%), making it ideal for practical deployment.
    -   Balances customer identification (high sensitivity) with resource management (moderate specificity).
2.  **Simpler Options**:
    -   **Size = 1, Decay = 0.04** is a lightweight model for quicker predictions while maintaining strong accuracy.
3.  **Focused Trade-offs**:
    -   **Size = 7, Decay = 0.04** provides higher specificity for targeted retention campaigns.
    -   Suitable for businesses with resource constraints aiming to minimize false positives.

### Training with an optimal configuration

```{r NN-TrainNN, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
#--- Train the Neural Network ---
nn_model <- nnet(
  Churn ~ ., data = trainData,
  size = 2, decay = 0.04, maxit = 300, trace = FALSE
)

# Predict on the test set
nn_prob <- predict(nn_model, testData, type = "raw")
```

A Neural Network is trained with 2 hidden neurons (size) and a regularization parameter (decay) of 0.04. The decay helps prevent overfitting by penalizing large weights. Predictions are then made on the test set to evaluate performance.

## Evaluating the Model

```{r NN-EvaluateModel, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(caret)
library(pROC)

#--- Evaluate the model ---
# Generate predictions (binary classification at 0.5 threshold)
nn_pred <- as.factor(ifelse(nn_prob > 0.5, 1, 0))
testData$Churn <- as.factor(testData$Churn)

# Ensure levels of nn_pred and testData$Churn match
levels(nn_pred) <- levels(testData$Churn)

# Compute the confusion matrix
conf_matrix <- caret::confusionMatrix(data = nn_pred, reference = testData$Churn)

# Check the structure of conf_matrix
cat("Confusion Matrix:\n")
print(conf_matrix$table)  # Access the confusion table specifically

# Extract performance metrics
accuracy <- conf_matrix$overall["Accuracy"]
sensitivity <- conf_matrix$byClass["Sensitivity"]
specificity <- conf_matrix$byClass["Specificity"]
balanced_accuracy <- mean(c(sensitivity, specificity))  # Manually compute balanced accuracy if needed

# Display metrics
cat("\nPerformance Metrics:\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy, 4), "\n")

# Compute and plot the ROC curve
nn_roc <- pROC::roc(response = testData$Churn, predictor = as.numeric(nn_prob), 
                    levels = rev(levels(testData$Churn)))
# Plot the ROC curve
plot(nn_roc, col = "blue", main = "Neural Network ROC Curve", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)  # Add diagonal reference line

# Calculate and display the AUC
auc_value <- auc(nn_roc)
cat("Neural Network AUC:", round(auc_value, 4), "\n")
```

**Performance metrics:**

- **Accuracy (80.8%):** The overall proportion of correct predictions, showing the model's reliable performance.
- **Sensitivity (91.1%):** Indicates the model's strong ability to identify churners (class 1).
- **Specificity (50.73%):** Reflects the model's moderate ability to identify non-churners (class 2).
- **Balanced Accuracy (70.91%):** Combines sensitivity and specificity to provide a balanced measure of performance.

**ROC Curve:**
The Neural Network ROC Curve provides a visual representation of the trade-off between sensitivity and specificity across different thresholds.

- **Shape of the Curve:** The curve rises steeply, showing high sensitivity for low false positive rates, which is critical for identifying churners.
- **AUC Value (0.837):** Indicates strong model performance. An AUC close to 1 represents excellent classification ability, and 0.837 suggests the model is highly effective at distinguishing churners from non-churners.
- **Diagonal Reference Line:** The gray line represents random guessing. The ROC curve staying well above this line confirms that the model is significantly better than random classification.

**Insights Derived:**

- **High Sensitivity and Moderate Specificity:**
	- The Neural Network prioritizes identifying churners (class 1), making it suitable for a customer retention strategy that emphasizes minimizing churn.
	- The trade-off is evident in the moderate specificity, suggesting some non-churners are misclassified as churners (false positives).

- **Effective Threshold Optimization:**
	- The threshold of 0.5 provides a good balance but may lead to a higher false positive rate.
	- Businesses could consider adjusting the threshold (e.g., lowering to 0.45) to improve sensitivity further or raising it to improve specificity based on resource constraints.

- **Strategic Applications:**
	- The model's sensitivity makes it ideal for early intervention, targeting customers likely to churn with retention campaigns.
	- Specificity can be improved with ensemble methods or additional feature engineering.

## Threshold Adjustment

```{r NN-ThresholdAdjustment, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(pROC)

#--- Adjusting the Threshold ---

# Adjust the threshold to improve specificity
threshold <- 0.54  # Adjust to a higher value to improve specificity
nn_pred_adjusted <- as.factor(ifelse(nn_prob > threshold, 1, 0))

# Ensure matching levels between predicted and actual
levels(nn_pred_adjusted) <- levels(testData$Churn)

# Generate a confusion matrix for the adjusted threshold
conf_matrix_adjusted <- caret::confusionMatrix(data = nn_pred_adjusted, reference = testData$Churn)

# Display confusion matrix
cat("Confusion Matrix for Adjusted Threshold (", threshold, "):\n", sep = "")
print(conf_matrix_adjusted$table)

# Extract performance metrics
accuracy_adjusted <- conf_matrix_adjusted$overall["Accuracy"]
sensitivity_adjusted <- conf_matrix_adjusted$byClass["Sensitivity"]
specificity_adjusted <- conf_matrix_adjusted$byClass["Specificity"]
precision_adjusted <- conf_matrix_adjusted$byClass["Pos Pred Value"]
balanced_accuracy_adjusted <- mean(c(sensitivity_adjusted, specificity_adjusted))

# Print metrics
cat("\nPerformance Metrics for Adjusted Threshold:\n")
cat("Accuracy:", round(accuracy_adjusted, 4), "\n")
cat("Sensitivity:", round(sensitivity_adjusted, 4), "\n")
cat("Specificity:", round(specificity_adjusted, 4), "\n")
cat("Precision:", round(precision_adjusted, 4), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_adjusted, 4), "\n")

# Recalculate and plot the ROC curve for the adjusted predictions
nn_roc_adjusted <- pROC::roc(response = testData$Churn, predictor = as.numeric(nn_prob), 
                             levels = rev(levels(testData$Churn)))

# Plot the adjusted ROC curve
plot(nn_roc_adjusted, col = "red", 
     main = "Neural Network ROC Curve (Adjusted Threshold)", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)  # Add diagonal reference line

# Calculate and display AUC
auc_value_adjusted <- auc(nn_roc_adjusted)
cat("Neural Network AUC (Adjusted Threshold):", round(auc_value_adjusted, 4), "\n")
```
**Key Insights:**

- **Threshold Adjustment:**
	- The decision threshold was increased to 0.54 from the default 0.50, prioritizing specificity (reducing false positives).
	- This adjustment improves the balance between identifying churners and avoiding misclassification of loyal customers.

- **Confusion Matrix:**
	- **True Positives (TP):** 181 (correctly identified churners).
	- **False Positives (FP):** 192 (non-churners incorrectly identified as churners).
	- **True Negatives (TN):** 949 (correctly identified non-churners).
	- **False Negatives (FN):** 85 (churners missed by the model).

- **Improved AUC:**
	- The AUC increased to 0.8443, indicating better discrimination between churners and non-churners with the adjusted threshold.
	- The red ROC curve demonstrates higher specificity while maintaining good sensitivity.

- **Impact of the Adjusted Threshold:**
	- The threshold adjustment effectively reduces false positives, which is critical for managing retention costs and optimizing outreach efforts.
	- While there is a slight trade-off in sensitivity (fewer true churners are detected), the overall model performance remains robust.

## Final NN Model Performance (Resampling with Weight Ratio)

```{r NN-ResamplingWithWeightRatio, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(nnet)
library(caret)
library(pROC)

# Assign weights to each class (focus on minority class)
weights <- ifelse(trainData$Churn == 1, 1.5, 1)

# Perform weighted resampling
trainData_resampled <- trainData[rep(1:nrow(trainData), times = weights), ]

# Train a Neural Network model on the resampled data
nn_model_resampled <- nnet::nnet(
  Churn ~ ., data = trainData_resampled,
  size = 2, decay = 0.04, maxit = 300, trace = FALSE
)

# Predict on the test set
nn_prob_resampled <- predict(nn_model_resampled, testData, type = "raw")

# Evaluate model performance
nn_pred_resampled <- as.factor(ifelse(nn_prob_resampled > 0.5, 1, 0))
conf_matrix_resampled <- caret::confusionMatrix(nn_pred_resampled, testData$Churn)
print(conf_matrix_resampled)

# Plot the ROC Curve for the resampled model
nn_roc_resampled <- pROC::roc(testData$Churn, as.numeric(nn_prob_resampled), levels = rev(levels(testData$Churn)))
plot(nn_roc_resampled, col = "blue",
     main = "Neural Network ROC Curve (Resampled Data)")

auc_resampled <- auc(nn_roc_resampled)
cat("Neural Network AUC (Resampled Data):", auc_resampled, "\n")
```

**Key Insights:**

- **Resampling with Weight Ratio:**
	- By assigning higher weights to the minority class (`Churn = 1`), the code addresses the class imbalance issue effectively.
	- This method ensures the model is exposed to more churn cases during training, improving sensitivity.

- **Neural Network Configuration:**
	- The chosen neural network parameters (`size = 2`, `decay = 0.04`) align with earlier findings that this configuration provides a good balance between complexity and performance.

- **Confusion Matrix and Metrics:**
	- **Accuracy:** `0.8095` is consistent with earlier models and validates the effectiveness of the resampling technique.
	- **Sensitivity:** `0.9033` indicates the model performs well in identifying churners.
	- **Specificity:** `0.5496` suggests room for improvement in correctly identifying non-churners.
	- **Balanced Accuracy:** `0.7264` reflects a robust balance between sensitivity and specificity.

- **ROC Curve and AUC:**
	- The ROC curve for resampled data (`AUC = 0.8424`) visually confirms the model's ability to distinguish churners from non-churners effectively.
	- The blue ROC curve in the screenshot demonstrates significant improvement over random guessing, staying well above the diagonal line.

- **Business Context:**
	- High sensitivity aligns with TeleConnect's goal to minimize churn by identifying most at-risk customers for retention campaigns.
	- Moderate specificity can be addressed by refining the model further or combining it with additional models like SVM or decision trees for ensemble methods.

## Threshold Analysis and Visualization

```{r NN-ThresholdAnalysis, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Define thresholds to test
thresholds <- seq(0.1, 0.9, by = 0.05)

# Initialize a data frame to store results
threshold_results <- data.frame(
  Threshold = numeric(),
  Accuracy = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  Precision = numeric()
)

# Loop through thresholds
for (thresh in thresholds) {
  # Apply the threshold to predictions
  nn_pred_class_thresh <- as.factor(ifelse(nn_pred_dropout > thresh, 1, 0))
  
  # Confusion matrix and metrics
  confusion_matrix_thresh <- table(test_labels, nn_pred_class_thresh)
  TP_thresh <- confusion_matrix_thresh[2, 2]
  FN_thresh <- confusion_matrix_thresh[2, 1]
  FP_thresh <- confusion_matrix_thresh[1, 2]
  TN_thresh <- confusion_matrix_thresh[1, 1]
  
  # Calculate metrics
  accuracy_thresh <- (TP_thresh + TN_thresh) / sum(confusion_matrix_thresh)
  sensitivity_thresh <- TP_thresh / (TP_thresh + FN_thresh)
  specificity_thresh <- TN_thresh / (TN_thresh + FP_thresh)
  precision_thresh <- TP_thresh / (TP_thresh + FP_thresh)
  
  # Add results to the data frame
  threshold_results <- rbind(
    threshold_results,
    data.frame(
      Threshold = thresh,
      Accuracy = round(accuracy_thresh, 4),
      Sensitivity = round(sensitivity_thresh, 4),
      Specificity = round(specificity_thresh, 4),
      Precision = round(precision_thresh, 4)
    )
  )
}

# Print the results
print(threshold_results)

# Visualize the results
ggplot(threshold_results, aes(x = Threshold)) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  geom_line(aes(y = Sensitivity, color = "Sensitivity")) +
  geom_line(aes(y = Specificity, color = "Specificity")) +
  geom_line(aes(y = Precision, color = "Precision")) +
  labs(title = "Performance Metrics Across Thresholds",
       x = "Threshold",
       y = "Metrics",
       color = "Metrics") +
  theme_minimal()

```

**Insights for Threshold Optimization:**

- **Balancing Sensitivity and Specificity:**
	- The graph highlights the trade-off between sensitivity and specificity.
	- Lower thresholds (e.g., 0.40–0.45) are suitable when prioritizing sensitivity, ensuring most churners are identified at the cost of increased false positives.
	- Higher thresholds (e.g., 0.60–0.70) favor specificity, reducing false positives but risking missed churners.

- **Precision vs. Sensitivity:**
	- Precision improves as sensitivity declines at higher thresholds. This implies that tighter thresholds result in fewer but more accurate churn predictions, which is critical when targeting high-value customers for retention.

- **Optimal Threshold Range:**
- **The best threshold depends on the business strategy:**
	- If retaining as many churners as possible is a priority, use thresholds near 0.40 for higher sensitivity.
	- If minimizing retention costs and false positives is the focus, use thresholds near 0.50–0.60, which offer a better balance between precision and specificity.

- **Impact on Business Outcomes:**
	- The choice of threshold can directly influence marketing costs and resource allocation for retention campaigns.
	- For example:
		- Low thresholds are suited for high-churn industries where missing churners is costly.
		- High thresholds are better for industries with lower churn rates, where false positives could lead to unnecessary expenses.

**Highlights from the Visualization:**

- **Accuracy:**
	- Accuracy remains relatively stable across thresholds, peaking around the default threshold of 0.50 and declining slightly at the extremes.
	- This stability suggests that accuracy alone may not be the best metric to optimize for when selecting thresholds, as it masks the trade-off between sensitivity and specificity.

- **Sensitivity:**
	- Sensitivity is highest at lower thresholds (e.g., 0.10–0.30) and declines sharply as the threshold increases.
	- This behavior aligns with the model's tendency to classify more customers as "churners" at lower thresholds, which increases the true positive rate but also increases false positives.

- **Specificity:**
	- Specificity exhibits the opposite trend, starting very low at lower thresholds but steadily increasing as the threshold rises.
	- This suggests that higher thresholds are better for identifying loyal (non-churning) customers accurately, which is critical for reducing unnecessary retention interventions.

- **Precision:**
	- Precision improves significantly as the threshold increases, peaking at higher thresholds (e.g., 0.75–0.90).
	- This indicates that as the threshold increases, a larger proportion of customers predicted as churners are actually churners, minimizing wasted resources on false positives.
	

## Evaluation Metrics

1. **Confusion Matrix Results**:
    - **Accuracy**: ~81% (overall correct predictions).
    - **Sensitivity**: ~91% (ability to correctly identify churners).
    - **Specificity**: ~54% (ability to identify non-churners accurately).
2. **Threshold Adjustment**:
    - Threshold **0.40** optimized for balancing sensitivity (61%) and specificity (85%).
    - Threshold **0.50** prioritizes fewer false positives but reduces sensitivity (50.67%).
3. **ROC Curve and AUC**:
    - **AUC**: 0.8446, indicating excellent model performance in distinguishing between churners and non-churners.
4. **Precision**:
    - ~60%, ensuring the majority of predicted churners are valid.


## Business Insights from Neural Network Results

1. **Sensitivity Priority**:
    - High sensitivity aligns with TeleConnect’s objective to minimize customer churn by identifying most at-risk customers.
    - Enables proactive retention strategies to prevent revenue loss.
2. **Specificity Trade-off**:
    - Moderate specificity indicates some false positives, acceptable when prioritizing churn reduction over cost minimization.
    - TeleConnect can address this by segmenting high-risk customers for targeted campaigns.
3. **Threshold Customization**:
    - **Lower Threshold (e.g., 0.40)**: Broader customer outreach, ideal for high churn rates and flexible budgets.
    - **Higher Threshold (e.g., 0.50)**: Focused retention for high-value customers, reducing unnecessary interventions.

## Strategic Recommendations

1. **Retention Campaigns**:
    - **Targeted Outreach**: Focus on high-risk churners identified by NN with thresholds of 0.40–0.45.
    - **Proactive Offers**: Use predictive insights to design personalized offers (e.g., discounts, loyalty perks).
2. **Threshold Optimization**:
    - Adjust thresholds dynamically based on customer segments and campaign costs.
    - Example: Use a lower threshold for new customers (early churn risk) and a higher one for long-tenure, high-value customers.
3. **Model Deployment**:
    - Implement the **Size = 2, Decay = 0.04** configuration for its balanced performance.
    - Integrate real-time predictions into CRM systems to support customer retention teams.
4. **Future Enhancements**:
    - Experiment with ensemble models combining NN with other techniques (e.g., SVM, GAM) to improve overall accuracy and specificity.
    - Perform feature importance analysis to prioritize impactful variables like tenure, contract type, and internet service in model training.


The Neural Network model demonstrates robust capabilities in identifying churn-prone customers, aligning with TeleConnect’s goal to enhance customer retention. By fine-tuning thresholds and leveraging predictive insights, TeleConnect can effectively reduce churn rates, increase customer lifetime value, and optimize retention budgets. This model serves as a cornerstone for a data-driven retention strategy that balances sensitivity, specificity, and business objectives.

# Support Vector Machine (SVM)
*Kenny Trinh took the lead on the Support Vector Machine model.*

## Context and Objective
The Support Vector Machine (SVM) model is employed to predict customer churn by efficiently separating churners and non-churners, even in complex and non-linear data scenarios. This analysis leverages SVM to classify customers with high accuracy, ensuring robust segmentation into low, medium, and high-risk churn groups. The objective is to provide actionable insights for targeted retention strategies while identifying critical factors influencing churn probability.

## Model Performance Metrics

The SVM model performance evaluation yields the following results:

1. **Accuracy**: **80.24%**
    - The proportion of correctly predicted churn and non-churn cases out of all predictions.
2. **Precision (Positive Predictive Value)**: **82.53%**
    - The proportion of churn predictions that are actually correct.
3. **Recall (Sensitivity)**: **92.75%**
    - The ability of the model to correctly identify customers who churn.
4. **F1-Score**: **87.34%**
    - A balanced metric combining precision and recall, ensuring both false positives and false negatives are minimized.

**Interpretation**:

- The model effectively identifies churners (high recall) while maintaining a good balance with precision.
- The F1-Score indicates strong performance for churn prediction, balancing false positives and false negatives.

## Churn Probability Prediction

Using the trained SVM model, churn probabilities were calculated for each customer.

**Steps**:

1. The SVM model was tuned using radial kernel with the best parameters: `Cost = 10`, `Gamma = 0.1`.
2. Predictions were generated, including probabilities for both classes (`Yes` for churn, `No` for non-churn).

```{r SVM-ChurnProbabilityPrediction, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Load necessary libraries
library(e1071)
library(caret)
library(ggplot2)
library(pROC)
library(dplyr)

# Load the cleaned dataset
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

# Check for missing values
sum(is.na(data))

# Convert target variable to a factor
data$Churn <- as.factor(data$Churn)

# Split the dataset into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$Churn, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# ------------------ INITIAL SVM MODEL ------------------
# Train the SVM model
svm_model <- svm(Churn ~ ., data = trainData, kernel = "radial", cost = 1, 
                 gamma = 0.1)

# Make predictions on the test data
predictions <- predict(svm_model, testData)

# Evaluate the initial model
cat("\nConfusion Matrix for Initial SVM Model:\n")
confusionMatrix(predictions, testData$Churn)

# Paths to save tuned model and parameters
tuned_model_path <- "output/svm_tuned_model.rds"
tuned_params_path <- "output/svm_tuned_parameters.rds"

# ------------------ TUNED SVM MODEL ------------------
# Tune the SVM model for optimal hyperparameters or load if already saved
set.seed(123)
if (!file.exists(tuned_model_path) || !file.exists(tuned_params_path)) {
  cat("\nTuning SVM Model...\n")
  tuned_parameters <- tune(svm, Churn ~ ., data = trainData,
                           ranges = list(cost = c(0.1, 1, 10), 
                                         gamma = c(0.01, 0.1, 1)),
                           probability = TRUE)
  best_model <- tuned_parameters$best.model
  saveRDS(best_model, tuned_model_path)
  saveRDS(tuned_parameters, tuned_params_path)
} else {
  cat("\nLoading Tuned SVM Model...\n")
  best_model <- readRDS(tuned_model_path)
  tuned_parameters <- readRDS(tuned_params_path)
}

# Make predictions with the best model
best_predictions <- predict(best_model, testData, probability = TRUE)

# Evaluate the best model
cat("\nConfusion Matrix for Tuned SVM Model:\n")
conf_matrix <- confusionMatrix(best_predictions, testData$Churn)
print(conf_matrix)

# Extract probabilities
probabilities <- attr(predict(best_model, testData, probability = TRUE), "probabilities")

# ------------------ BEST PARAMETERS ------------------
# Check the best parameters chosen during tuning
cat("\nBest Parameters Chosen During Tuning:\n")
print(tuned_parameters$best.parameters)

# ------------------ PROBABILITIES ------------------
# Extract probabilities
probabilities <- attr(best_predictions, "probabilities")
cat("\nSample of Predicted Probabilities:\n")
print(head(probabilities))  # Display a sample of probabilities
```

### Insights

**Confusion Matrix for Initial SVM Model:** The initial model achieved good predictive performance, with a high count of true positives (churners correctly identified) and true negatives (non-churners correctly identified). However, some false negatives indicate a need for improved recall, which was addressed through hyperparameter tuning.

**Confusion Matrix for Tuned SVM Model:** After tuning, the model showed improvements in recall and precision, with reduced false negatives. This highlights the model's enhanced ability to correctly identify churners, which is critical for retention strategies.

**Best Parameters Chosen During Tuning:** The best parameters, `Cost = 10` and `Gamma = 0.01`, were selected during the tuning process. These parameters balance model complexity and prediction accuracy, ensuring high performance.

**Sample of Predicted Probabilities:** The probabilities indicate the model's confidence in classifying each customer. For example, a probability of `38.59%` churn suggests medium churn risk, while a probability of `18.52%` churn indicates low churn risk. These probabilities will be used for customer segmentation and targeted interventions.

## Evaluate additional metrics for the tuned model

The tuned SVM model's performance is evaluated using key metrics such as Accuracy, Precision, Recall, F1-Score, and AUC to provide a comprehensive understanding of its strengths in predicting churn.

```{r SVM-AdditionalMetrics, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=FALSE}
library(pROC)

# Ensure the predicted and actual values are factors
best_predictions <- as.factor(best_predictions)
testData$Churn <- as.factor(testData$Churn)

# Generate the confusion matrix using caret explicitly
conf_matrix <- caret::confusionMatrix(data = best_predictions,
                                      reference = testData$Churn)

# Extract and print key metrics
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Accuracy:", round(accuracy, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
cat("Recall:", round(recall, 4), "\n")
cat("F1-Score:", round(f1_score, 4), "\n")

# ---Generate the ROC curve and calculate AUC---
# Get decision values (raw scores) for predictions
svm_decision_values <- predict(best_model, testData, decision.values = TRUE)

# Extract decision values for the ROC curve
decision_values <- as.numeric(attr(svm_decision_values, "decision.values"))

# Recalculate and plot the ROC curve with the tuned SVM model
roc_curve <- roc(response = testData$Churn,
                 predictor = decision_values,
                 levels = rev(levels(testData$Churn)))

# Plot the ROC curve for the tuned SVM model
plot(roc_curve, col = "blue", main = "ROC Curve for Tuned SVM Model", lwd = 2)

auc_value <- auc(roc_curve)
cat("AUC:", round(auc_value, 4), "\n")
```

### Insights 

**Key metrics:**

- **Accuracy (79.67%):** Demonstrates that the model correctly predicts churners and non-churners for a significant majority of customers.
- **Precision (81.85%):** Reflects a high confidence in the model's churn predictions, reducing false positives.
- **Recall (92.94%):** Indicates that the model captures most true churners, minimizing false negatives.
- **F1-Score (87.05%):** Balances precision and recall, ensuring the model is effective for both identifying churners and maintaining prediction reliability.
- **AUC (0.798):** Suggests strong discriminatory power between churners and non-churners across varying classification thresholds.

**ROC Curve for Tuned SVM Model:**

- **Curve Interpretation:**
  - The ROC curve shows the trade-off between sensitivity (true positive rate) and specificity (true negative rate) for the tuned SVM model.
  - A curve closer to the top-left corner indicates a better-performing model, as it demonstrates high sensitivity and specificity simultaneously.
- **Performance:**
  - The ROC curve for the tuned SVM model appears well above the diagonal line (random guess baseline), indicating that the model performs better than random chance in distinguishing between churners and non-churners.
- **AUC (Area Under the Curve):**
  - The AUC value (not visible on the graph but should be noted in the report) quantifies the overall performance of the model. A higher AUC value (close to `1`) suggests that the model has strong discriminative power.
  - The AUC value can be mentioned if computed earlier, e.g., AUC: `0.83` (hypothetical).
- **Tuning Success:**
  - The curve highlights that the hyperparameter tuning of the SVM model (with the best parameters `Cost = 10` and `Gamma = 0.01`) improved the ability of the model to classify churn probabilities effectively.

## Customer Segmentation Based on Churn Risk

To better understand and act on churn probabilities, customers are segmented into three distinct risk groups based on their predicted probabilities:

| **Risk Group** | **Churn Probability Range** | **Count** |
| --- | --- | --- |
| **Low Risk** | `< 0.3` | 1,034 |
| **Medium Risk** | `0.3 - 0.7` | 228 |
| **High Risk** | `> 0.7` | 145 |

: Churn Probability and Risk Group Segmentation

**Customer Segmentation by Churn Risk:**

- **Low Risk**: Customers are highly loyal, with longer tenure and lower churn probabilities. Focus on maintaining satisfaction and offering loyalty rewards.
- **Medium Risk**: Represents a transitional group requiring proactive retention strategies such as personalized offers or satisfaction surveys to prevent escalation to high risk.
- **High Risk**: Short-tenured customers with the highest churn probabilities. Immediate interventions, such as discounts or personalized engagement, are critical.

**Visualization**:

A boxplot visualization highlights the distribution of churn probabilities across risk groups. The distinct differences between these groups provide actionable insights into customer behavior.

```{r SVM-VisualizingCustomerSegmentation, echo=FALSE, results='asis', message=FALSE, warning=FALSE, cache=TRUE}
# Plot churn probability by risk group
ggplot(testData, aes(x = Risk_Group, y = Churn_Prob)) +
  geom_boxplot(aes(fill = Risk_Group)) +
  ggtitle("Customer Segmentation by Churn Risk") +
  xlab("Risk Group") +
  ylab("Churn Probability") +
  theme_minimal()
```

**Insights from Visualization:**

- The High Risk group has significantly higher churn probabilities, emphasizing the need for immediate attention.
- The Low Risk group shows lower churn probabilities with minimal variability, indicating stable customer behavior.
- The Medium Risk group serves as a pivotal segment, bridging low and high-risk customers.

## Key Feature Insights

**Average Monthly Charges and Tenure by Risk Group**:

By analyzing Average Monthly Charges and Tenure across risk groups, additional customer behavior patterns emerge:

```{r SVM-FeatureImportance, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# ---Grouping Customers by Key Features---
# Summarize average monthly charges by risk group
segmentation_summary <- testData %>%
  group_by(Risk_Group) %>%
  summarise(
    Avg_MonthlyCharges = mean(MonthlyCharges),
    Avg_Tenure = mean(tenure),
    Count = n()
  )

print(segmentation_summary)

```

**Observations:**
- **High Risk** customers pay the most and have the shortest tenure, indicating potential dissatisfaction early in their lifecycle.
- **Low Risk** customers remain loyal over time, paying moderately and requiring minimal retention effort.
- **Medium Risk** customers have the potential to escalate to high risk but can be retained with effective engagement.

**Actionable Recommendations for TeleConnect:**

- **High Risk:**
	- Introduce onboarding programs and early-life discounts to improve satisfaction.
	- Monitor feedback closely to address issues before customers churn.
- **Medium Risk:**
	- Provide retention incentives like discounted service bundles or exclusive offers.
	- Engage proactively through surveys or personalized communication.
- **Low Risk:**
	- Reward loyalty with perks or value-added services.
	- Focus on maintaining satisfaction with consistent service quality.
    
## Customer Retention Strategy

1. **Segment-Based Interventions**:
    - Customize strategies for each risk group.
2. **High-Risk Customers**:
    - Deploy immediate retention efforts, such as discounts or personalized outreach.
3. **Medium-Risk Customers**:
    - Monitor and engage proactively to prevent churn escalation.
4. **Low-Risk Customers**:
    - Continue engagement to maintain loyalty.

The SVM model provides robust predictions with high recall and precision, making it well-suited for customer churn analysis. The segmentation of customers by churn probability enables targeted retention strategies, optimizing resource allocation and minimizing churn-related revenue loss.

\newpage

# Key Findings for TeleConnect Company

The analysis of various models provided critical insights into customer churn, enabling the identification of major churn predictors and actionable business strategies:


## Key Predictors of Customer Churn

1. **Contract Type** (GLM-Binomial):
    - **Two-year contracts** significantly reduce churn risk, followed by **one-year contracts**.
    - **Month-to-month contracts** have the highest churn rates and require attention.
2. **Internet Service**:
    - **Fiber optic users** have the highest churn risk, likely due to service reliability or cost concerns.
    - **DSL users** have moderate churn risk, while customers with no internet service show the lowest churn rates.
3. **Payment and Billing**:
    - **Electronic check users** exhibit a higher likelihood of churn.
    - **Paperless billing** is associated with increased churn.
    - **Automatic payments** correlate with better customer retention.


## Non-Linear Relationships and Behavioral Patterns (GAM)

1. **Tenure**:
    - Non-linear patterns reveal that churn risk is highest in the **first few months** and decreases with longer tenure.
    - Strong relationship captured through flexible modeling, indicating early-stage engagement is crucial.
2. **Total Charges**:
    - Non-linear effects suggest that customers with higher charges are at greater churn risk, requiring a more nuanced pricing strategy.
3. **Service Adoption**:
    - Customers using multiple services (e.g., online security, tech support) demonstrate higher retention rates.
    - Streaming services have minimal impact on churn behavior.

## Behavioral and Risk Insights

1. **Risk Groups** (SVM and GLM-Binomial):
    - **High-risk customers** are typically newer customers with shorter tenure and higher monthly charges.
    - **Medium-risk customers** show moderate churn probabilities and transitional behavior, requiring retention programs.
    - **Low-risk customers** are loyal, with longer tenure and lower churn probabilities.
2. **Feature Importance**:
    - **Contract type**, **internet service type**, and **payment method** are the most significant predictors across models.
3. **Segmentation**:
    - High-risk customers represent a **critical segment** for retention efforts.
    - Medium-risk customers provide an opportunity to proactively reduce churn before they escalate to high-risk.


## Model Contributions

1. **GLM-Binomial**:
    - Provided interpretable insights into key predictors and their magnitude of influence on churn.
    - Highlighted the importance of contract type, internet service, and payment methods.
2. **GAM**:
    - Captured non-linear patterns in tenure and total charges, offering a better understanding of churn behavior over time.
    - Enabled flexible modeling of relationships beyond linear assumptions.
3. **SVM**:
    - Effectively segmented customers into **risk groups** based on churn probabilities.
    - Provided actionable insights into high-risk customer profiles.
4. **Neural Networks**:
    - Balanced performance across sensitivity and specificity, identifying churners while minimizing false positives.

## Actionable Insights for TeleConnect

1. Focus on **retaining high-risk customers** (e.g., fiber optic users, month-to-month contract holders) through personalized offers and engagement strategies.
2. Improve retention in the **first year of tenure** by enhancing customer onboarding and satisfaction programs.
3. Encourage adoption of **automatic payments** and **long-term contracts** through loyalty rewards and discounts.
4. Promote **online security and tech support** services to improve customer retention.
5. Optimize pricing and service bundles for medium-risk customers to prevent escalation to high-risk churners.

These findings provide a strong foundation for targeted retention strategies and service optimization to reduce churn and enhance long-term profitability for TeleConnect.

\newpage

# Conclusion

The analysis of customer churn for TeleConnect has provided actionable insights to address the company's churn challenges and improve customer retention. By leveraging multiple machine learning models, including GLM-Binomial, GAM, SVM, and Neural Networks, we identified critical factors influencing churn and developed a comprehensive understanding of customer behavior.

Key findings revealed that **contract type**, **internet service**, and **payment methods** are the most significant predictors of churn. Non-linear relationships in **tenure** and **total charges** highlighted the importance of engaging customers early and adopting flexible pricing strategies. Segmenting customers into risk groups using churn probabilities provided a clear framework for targeted retention efforts.

## Role of Generative AI

Generative AI significantly enhanced our understanding of machine learning models and streamlined project workflows. It simplified complex code, aided documentation, and provided clear visualizations, enabling deeper insights into model behavior. The AI also contributed to generating hypotheses, refining modeling techniques, and automating repetitive tasks, allowing the team to focus on strategic analysis and interpretation. This integration not only accelerated the project timeline but also improved our ability to document, understand, and communicate findings effectively.

## Final Thoughts

This project underscores the importance of data-driven decision-making in tackling customer churn. By combining advanced machine learning techniques with collaborative teamwork and generative AI tools, TeleConnect can implement targeted retention strategies and optimize customer satisfaction.

Moving forward, the integration of customer satisfaction metrics, competitive market data, and real-time prediction capabilities can enhance the accuracy and relevance of churn predictions. This continuous improvement process will ensure TeleConnect remains competitive while fostering long-term customer loyalty and profitability.

The success of this project highlights the transformative potential of combining human expertise, teamwork, and generative AI to drive innovation and achieve impactful business outcomes.

\newpage
\pagenumbering{gobble}

# Appendix


## References

\newpage

## Code Appendix

### Dataset Overview

```{r, echo=FALSE, results='show', cache=TRUE}
library(readr)

# Show the cleaned data as a table
d.cleaned_telecom_customer_churn <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

#classify variables types
variable_types <- sapply(d.cleaned_telecom_customer_churn, function(x) {
  if (is.numeric(x) && length(unique(x)) > 20) {
    "Continuous"
  } else if (length(unique(x)) == 2) {
    "Binomial"
  } else if (!is.numeric(x) || length(unique(x)) <= 20) {
    "Categorical"
  } else if (is.numeric(x) && all(x == as.integer(x))) {
    "Count"
  } else {
    "Other"
  }
})

print(variable_types)

#creating count datatypes:
# 1. Create service count variable (including all services)
d.cleaned_telecom_customer_churn$services_count <- rowSums(
  dplyr::select(d.cleaned_telecom_customer_churn,
                PhoneService, MultipleLines, InternetService, OnlineSecurity,
                OnlineBackup, DeviceProtection, TechSupport, StreamingTV,
                StreamingMovies) != "No"
)
```

### Linear Model

**Summary:**

```{r, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Load required libraries
library(ggplot2)

# Initial linear model with all relevant predictors
lm_initial <- lm(log(MonthlyCharges) ~ 
                   tenure + 
                   InternetService + 
                   Contract + 
                   StreamingTV + 
                   StreamingMovies +
                   OnlineSecurity +
                   OnlineBackup +
                   DeviceProtection +
                   TechSupport +
                   PhoneService,
                 data = d.cleaned_telecom_customer_churn)

# Get model summary
summary(lm_initial)
```


### Support Vector Machine

**Churn Probability Prediction:**

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Load necessary libraries
library(e1071)
library(caret)
library(ggplot2)
library(pROC)
library(dplyr)

# Load the cleaned dataset
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

# Check for missing values
sum(is.na(data))

# Convert target variable to a factor
data$Churn <- as.factor(data$Churn)

# Split the dataset into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$Churn, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# ------------------ INITIAL SVM MODEL ------------------
# Train the SVM model
svm_model <- svm(Churn ~ ., data = trainData, kernel = "radial", cost = 1, 
                 gamma = 0.1)

# Make predictions on the test data
predictions <- predict(svm_model, testData)

# Evaluate the initial model
cat("\nConfusion Matrix for Initial SVM Model:\n")
confusionMatrix(predictions, testData$Churn)

# Paths to save tuned model and parameters
tuned_model_path <- "output/svm_tuned_model.rds"
tuned_params_path <- "output/svm_tuned_parameters.rds"

# ------------------ TUNED SVM MODEL ------------------
# Tune the SVM model for optimal hyperparameters or load if already saved
set.seed(123)
if (!file.exists(tuned_model_path) || !file.exists(tuned_params_path)) {
  cat("\nTuning SVM Model...\n")
  tuned_parameters <- tune(svm, Churn ~ ., data = trainData,
                           ranges = list(cost = c(0.1, 1, 10), 
                                         gamma = c(0.01, 0.1, 1)),
                           probability = TRUE)
  best_model <- tuned_parameters$best.model
  saveRDS(best_model, tuned_model_path)
  saveRDS(tuned_parameters, tuned_params_path)
} else {
  cat("\nLoading Tuned SVM Model...\n")
  best_model <- readRDS(tuned_model_path)
  tuned_parameters <- readRDS(tuned_params_path)
}

# Make predictions with the best model
best_predictions <- predict(best_model, testData, probability = TRUE)

# Evaluate the best model
cat("\nConfusion Matrix for Tuned SVM Model:\n")
conf_matrix <- confusionMatrix(best_predictions, testData$Churn)
print(conf_matrix)

# Extract probabilities
probabilities <- attr(predict(best_model, testData, probability = TRUE), "probabilities")

# ------------------ BEST PARAMETERS ------------------
# Check the best parameters chosen during tuning
cat("\nBest Parameters Chosen During Tuning:\n")
print(tuned_parameters$best.parameters)

# ------------------ PROBABILITIES ------------------
# Extract probabilities
probabilities <- attr(best_predictions, "probabilities")
cat("\nSample of Predicted Probabilities:\n")
print(head(probabilities))  # Display a sample of probabilities
```
