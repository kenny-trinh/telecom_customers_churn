---
title: "Telecom Customer Churn Prediction Analysis"
author:
  - "Kenny Trinh"
  - "Sabin Pun"
  - "Sarp Koc"
date: "2025-01-10"
output:
  pdf_document:
    toc: true
    fig_caption: true
    number_sections: true
    highlight: pygments
header-includes:
  - \pagenumbering{gobble}
---

\newpage
\pagenumbering{arabic}

```{r Setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 6, fig.height = 2, fig.align = "center" )

# List of required packages
required_packages <- c(
  "readr",
  'dplyr',
  'tidyverse',
  'caret',
  'nnet',
  'pROC',
  'ggplot2',
  'corrplot',
  'RSNNS',
  'e1071',
  'mgcv',
  'gridExtra',
  "tinytex",
  "stringr",
  "tidyr",
  "xfun"
)

# Function to check and install missing packages
install_if_missing <- function(packages) {
  missing_packages <- packages[!packages %in% installed.packages()[, "Package"]]
  if (length(missing_packages) > 0) {
    install.packages(missing_packages)
  }
}

# Install any missing packages
install_if_missing(required_packages)

# Load all packages
lapply(required_packages, library, character.only = TRUE)

# Define data paths
data_dir <- "data/"
raw_data_dir <- paste0(data_dir, "raw/")
cleaned_data_dir <- paste0(data_dir, "cleaned/")
output_dir <- "output/"

# Create data directories if they don't exist
dir.create(data_dir, showWarnings = FALSE)
dir.create(raw_data_dir, showWarnings = FALSE)
dir.create(cleaned_data_dir, showWarnings = FALSE)

# Create output directory if it doesn't exist
dir.create(output_dir, showWarnings = FALSE)
```

# A Multi-Model Machine Learning Approach

## Executive Summary

This analysis employs multiple machine learning approaches to understand customer behavior and predict churn in the telecommunications sector. By analyzing a comprehensive dataset of customer information, we provide actionable insights for improving customer retention and service optimization.

## Business Context: The Story of "TeleConnect"

Meet **TeleConnect**, a mid-sized telecommunications company striving to maintain its foothold in a fiercely competitive market. Over the past year, the company has faced growing challenges: increased customer churn, stagnant revenue growth, and difficulty in predicting which services drive customer satisfaction and retention.

### The Challenge

TeleConnect's problems are multifaceted:

-   **High Churn Rates**: Nearly 20% of their customer base leaves each year, especially those on month-to-month contracts.
-   **Price Sensitivity**: Customers complain about high monthly charges, but TeleConnect lacks clarity on whether price adjustments would help.
-   **Service Bundling**: While TeleConnect offers multiple services—Internet, Streaming, Security—adoption patterns remain unclear. Are these services increasing customer value, or are they simply an added cost?

TeleConnect’s leadership knows the stakes: acquiring a new customer costs significantly more than retaining an existing one. Yet, without data-driven insights, their current retention strategies feel like guesswork.

**Why Machine Learning?**

To address these challenges, TeleConnect has embraced a multi-model machine learning approach, leveraging advanced analytics to transform their operations. By applying complementary models tailored to different aspects of customer behavior, TeleConnect seeks to:

1.  **Predict Monthly Charges**: Understand the drivers of customer spending to design smarter pricing strategies and optimize service bundles.
2.  **Analyze Customer Tenure**: Uncover patterns in contract types, payment methods, and service adoption that influence the length of customer relationships.
3.  **Forecast Churn**: Identify at-risk customers proactively and implement targeted retention efforts.
4.  **Segment Customers by Risk**: Group customers into risk categories to prioritize retention strategies effectively.

**The Opportunity**

By integrating machine learning models into its decision-making processes, TeleConnect can:

- **Reduce churn** by predicting at-risk customers and proactively addressing their concerns.
- **Maximize revenue** through optimized pricing and smarter service bundling.
- **Improve retention** by understanding what drives tenure and customer satisfaction.

## Dataset Overview

The dataset encompasses comprehensive customer information including:

To categorize the variables in your dataset into **continuous**, **categorical**, **count**, and **binomial**, let’s analyze them step by step:

```{r DatasetOverview, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=FALSE}
library(readr)

# Show the cleaned data as a table
d.cleaned_telecom_customer_churn <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

#classify variables types
variable_types <- sapply(d.cleaned_telecom_customer_churn, function(x) {
  if (is.numeric(x) && length(unique(x)) > 20) {
    "Continuous"
  } else if (length(unique(x)) == 2) {
    "Binomial"
  } else if (!is.numeric(x) || length(unique(x)) <= 20) {
    "Categorical"
  } else if (is.numeric(x) && all(x == as.integer(x))) {
    "Count"
  } else {
    "Other"
  }
})

print(variable_types)

#creating count datatypes:
# 1. Create service count variable (including all services)
d.cleaned_telecom_customer_churn$services_count <- rowSums(
  dplyr::select(d.cleaned_telecom_customer_churn,
                PhoneService, MultipleLines, InternetService, OnlineSecurity,
                OnlineBackup, DeviceProtection, TechSupport, StreamingTV,
                StreamingMovies) != "No"
)
```

**1. Continuous Variables:**

-   `tenure` (Number of months the customer has been with the company)
-   `MonthlyCharges` (Monthly charge in dollars)
-   `TotalCharges` (Cumulative charges in dollars)

**2. Categorical Variables:**

-   `MultipleLines` (No phone service/Yes/No)
-   `InternetService` (DSL/Fiber optic/No)
-   `OnlineSecurity` (No internet service/Yes/No)
-   `OnlineBackup` (No internet service/Yes/No)
-   `DeviceProtection` (No internet service/Yes/No)
-   `TechSupport` (No internet service/Yes/No)
-   `StreamingTV` (No internet service/Yes/No)
-   `StreamingMovies` (No internet service/Yes/No)
-   `Contract` (Month-to-month/One year/Two year)
-   `PaymentMethod` (Electronic check/Mailed check/Bank transfer/Credit card)

**3. Binomial Variables:**

-   `gender` (Male/Female)
-   `SeniorCitizen` (0/1)
-   `Partner` (Yes/No)
-   `PhoneService` (Yes/No)
-   `PaperlessBilling` (Yes/No)
-   `Churn` (Yes/No - Target variable)

### Key Variables and Their Business Significance

1.  **Critical Predictors**
    -   Tenure: Indicates customer loyalty and relationship duration
    -   Contract Type: Reflects commitment level
    -   Monthly Charges: Represents service value and potential price sensitivity
    -   Internet Service: Core service offering affecting overall satisfaction
2.  **Service Usage Indicators**
    -   Multiple service subscriptions suggest higher customer engagement
    -   Security and support services indicate value-added service adoption
    -   Streaming services usage reflects modern consumption patterns
3.  **Financial Metrics**
    -   Average monthly charges: \$64.76
    -   Payment methods diversity indicates billing flexibility
    -   Relationship between charges and service subscriptions

# Exploratory Data Analysis

The exploratory data analysis focuses on understanding customer churn, service adoption, and tenure patterns. This section includes visual and statistical analysis to identify trends and factors influencing churn and customer behavior.

**Data Overview:**

The dataset contains 7,043 customers with key features like:
- Demographics (Tenure, Services Count)
- Contract & Payment details (Contract Type, MonthlyCharges)
- Churn Status (Target Variable)

**Log-Transformation of Charges (MonthlyCharges & TotalCharges)**

```{r EDA-Histograms-LogTransform, echo=FALSE, results='show', fig.width = 8, fig.height = 5, warning=FALSE, cache=TRUE}
# Load required libraries if not already loaded
library(ggplot2)
library(gridExtra)
library(dplyr)
library(corrplot)

# Apply log transformation
d.cleaned_telecom_customer_churn <- d.cleaned_telecom_customer_churn %>%
  mutate(log_MonthlyCharges = log(MonthlyCharges + 1),
         log_TotalCharges = log(TotalCharges + 1))

# Histograms before and after log transformation
p1 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = MonthlyCharges)) + 
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  ggtitle("Before Log Transformation (MonthlyCharges)")

p2 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = log_MonthlyCharges)) + 
  geom_histogram(bins = 30, fill = "darkred", alpha = 0.7) +
  ggtitle("After Log Transformation (MonthlyCharges)")

p3 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = TotalCharges)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  ggtitle("Before Log Transformation (TotalCharges)")

p4 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = log_TotalCharges)) + 
  geom_histogram(bins = 30, fill = "red", alpha = 0.7) +
  ggtitle("After Log Transformation (TotalCharges)")

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

- TotalCharges and MonthlyCharges were right-skewed, requiring log-transformation.
- After transformation, the distributions became more normal, improving model assumptions

**Key Findings from EDA**

```{r EDA-Boxplots, echo=FALSE, results='show', fig.width = 8, fig.height = 3, warning=FALSE, cache=TRUE}

# Churn distribution
churn_dist <- ggplot(d.cleaned_telecom_customer_churn, aes(x = Churn, fill = Churn)) +
  geom_bar() +
  geom_text(stat='count', aes(label=..count..), position=position_dodge(width=0.9), vjust=-0.5) +
  theme_minimal() +
  labs(title = "Distribution of Customer Churn", x = "Churn Status", y = "Number of Customers")

# Churn Rate by Contract Type:
contract_churn <- ggplot(d.cleaned_telecom_customer_churn, aes(x = Contract, fill = Churn)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Churn Rate by Contract Type", x = "Contract Type", y = "Proportion")



#Monthly Charages vs Tenure
charges_tenure <- ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure, y = MonthlyCharges, color = Churn)) +
  geom_smooth(method = "loess", se = TRUE) + 
  theme_minimal() +
  labs(title = "Monthly Charges vs Tenure (Smoothed)", x = "Tenure (months)", y = "Monthly Charges ($)")


# Arrange all plots in a grid: Bar plots on the left, Line plot on the right
grid.arrange(churn_dist, contract_churn, charges_tenure, 
             layout_matrix = rbind(c(1,2), 
                                   c(3,3)))

```

1. Churn Distribution
  - 26.5% of customers churned (1,869 out of 7,043).
  - Class imbalance exists (need to consider resampling for modeling).
2. Contract Type & Churn Relationship
  - Month-to-month contracts have the highest churn rate (42.7%).
  - Two-year contracts have the lowest churn rate (11.2%).
3.Relationship Between Monthly Charges & Tenure
  - Customers with lower tenure & high MonthlyCharges churn more frequently.
  - The relationship is non-linear, justifying using Generalized Additive Models(GAMs).
  
**Correlation Analysis with numeric variables **

```{r EDA-CorrelationMatrix, echo=FALSE, results='show', fig.width = 5, fig.height = 5, warning=FALSE, cache=TRUE}
# Selecting numeric variables
numeric_vars <- d.cleaned_telecom_customer_churn %>%
  select(tenure, log_MonthlyCharges, log_TotalCharges, services_count) %>%
  na.omit()

# Correlation matrix
corr_matrix <- cor(numeric_vars)

# Heatmap
corrplot(corr_matrix, method = "color", type = "upper")

```

**Correlation Analysis**

- TotalCharges and Tenure have a very high correlation (r $\approx$ 0.98), meaning one could be removed in some models to avoid multicollinearity.

Based on these findings, we proceed to the methodology section, where we implement various models to predict churn and evaluate their effectiveness.

# Methodology and Model Selection

Our analysis employs six complementary machine learning approaches, each chosen for specific analytical capabilities:

Our analysis employs a comprehensive suite of models, each chosen for specific analytical capabilities:

In our analysis, we employed the following machine learning approaches:

1.  **Linear Model (LM)**: To predict monthly charges and understand service impacts
2.  **GLM-Poisson**: To analyze customer tenure patterns and retention factors
3.  **GLM-Binomial**: To predict customer churn probability
4.  **Generalized Additive Model (GAM)**: To capture non-linear relationships in customer behavior
5.  **Neural Network (NN)**: To recognize complex patterns in customer churn
6.  **Support Vector Machine (SVM)**: To segment customers based on churn risk and grouping them.

\newpage

# Linear Model (LM)
*Sabin Pun took the lead on the linear model.*

To address **TeleConnect’s price sensitivity challenge**, we began by building a **Linear Model (LM)** to understand the key drivers of MonthlyCharges, we first build a Linear Model (LM) as a baseline approach. This helps TeleConnect analyze the impact of various customer attributes on pricing and service bundling

The response variable for this linear model is:

- **MonthlyCharges**: The amount customers pay per month(continuous)

The predictors used are:
1.  **Tenure**: Number of months the customer has been with the company.
2.  **Contract:** Type of contract( Month to Month, one year, two year)
3.  **InternetService**: Type of internet service (DSL, Fiber optic, No internet service).
4.  **StreamingTV**: Whether customers have streaming TV services (Yes/No).
5.  **PhoneService**: Whether customers have a phone service (Yes/No).
6.  **MultipleLines**: Whether customers have multiple phone lines (Yes/No).

Linear Model to predict monthly charges:

```{r LM-Model, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Load required libraries
library(ggplot2)

# Initial linear model with all relevant predictors
lm_initial <- lm(log(MonthlyCharges) ~ 
                   tenure + 
                   InternetService + 
                   Contract + 
                   StreamingTV + 
                   StreamingMovies +
                   OnlineSecurity +
                   OnlineBackup +
                   DeviceProtection +
                   TechSupport +
                   PhoneService,
                 data = d.cleaned_telecom_customer_churn)

# Get model summary
summary(lm_initial)
```

**Coefficient Interpretation**:

| **Predictor** | **Effect on Log(Monthly Charges)** |
| --- | --- |
| **Tenure** | `+0.00064` per month |
| **Internet Service (Fiber Optic)** | `+0.347` (**+41.5%**) |
| **Internet Service (No Service)** | `-0.863` (**-57.8%**) |
| **Contract (Two Years)** | `+0.00882` (**+9.2%**) |
| **Additional Services** | **Increase charges by 6% - 55%** |
 
: Coefficient interpretation for the linear model

**Interpretation:**

- **Tenure**: Longer tenure slightly increases monthly charges.
- **Internet Service**: Fiber optic users pay 41.5% more than DSL users, while customers with no internet service pay 57.8% less.
- **Contract Type**: Two-year contracts increase monthly charges by 9.2%.
- **Additional Services (includes Streaming, Security, Backup, Tech Support, Device Protection and Phone Service)**: Customers with multiple services pay 6% to 55% more, with Phone Service having the most significant impact.


## Visualizing the main effects

**Monthly Charges against Tenure:**/ 
**Monthly Charges vs Tenure:**

```{r LM-Visualizations, echo=FALSE, results='show', cache=TRUE}
# Plot 1: Log Monthly Charges vs Tenure by Internet Service
plot1 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure, y = log(MonthlyCharges), color = InternetService)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "loess", se = TRUE) + 
  theme_minimal() +
  labs(title = "Log Monthly Charges vs Tenure by Internet Service",
       x = "Tenure (months)", y = "Log Monthly Charges")

# Plot 2: Log Monthly Charges vs Tenure by Contract Type
plot2 <- ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure, y = log(MonthlyCharges), color = Contract)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "loess", se = TRUE) + 
  theme_minimal() +
  labs(title = "Log Monthly Charges vs Tenure by Contract Type",
       x = "Tenure (months)", y = "Log Monthly Charges")

# Arrange plots in grid view
grid.arrange(plot1, plot2, nrow = 2)
```

**Limitations of the Linear Model** 
While the linear model provides useful insights, it assumes a strictly linear relationship between features and Monthly Charges. However, as observed, some results—such as contract type pricing—contradict expectations.The visualizations indicate non-linear relationships between Tenure and Log Monthly Charges. This suggests that contract pricing follows a non-linear trend, which cannot be captured effectively by a simple linear model

- Contract types & internet services exhibit distinct trends, suggesting interactions that LM cannot fully capture.
- LOESS smoothers in the plots show curved trends, implying that a Generalized Additive Model (GAM) or non-linear methods (SVM, Neural Networks) might be better suited.

Therefore, while LM helps interpret key relationships, we move forward with GLMs, GAMs, and advanced models to improve predictive accuracy.

\newpage

# GLM-Poisson Model
*Sabin Pun took the lead on the GLM-Poisson model.*

To address **TeleConnect’s challenge of understanding customer retention**, we utilized a **GLM-Poisson model** to analyze the factors influencing **tenure**—the number of months a customer remains with the company. By identifying the key drivers of tenure, this model helps TeleConnect develop targeted retention strategies.

The response variable for Poisson regression must be **count data** (non-negative integers).

**Tenure** (count of months as a customer).

Include relevant predictors, such as:

-   `Contract`: Longer contracts may correlate with higher tenure.
-   `InternetService`: Different internet services might drive customer loyalty.
-   `PaymentMethod`: Certain payment methods might affect tenure (e.g., automatic payments may reduce churn).
-   `StreamingTV` and `StreamingMovies`: Value-added services could contribute to customer retention.
-   `SeniorCitizen`, `Partner`, `Dependents`: Demographics that might influence tenure.

Exploring the Data:

```{r Poisson-DataExploration, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Load required libraries
library(ggplot2)

ggplot(d.cleaned_telecom_customer_churn, aes(x = tenure)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Tenure",
       x = "Tenure (Months)",
       y = "Frequency") +
  theme_minimal()
```

-   There’s a high frequency of customers with **tenure close to 0 months**.
-   There’s another peak at **tenure near 72 months**, likely customers who have been with the company long-term.

## Fit Poisson regression

```{r Poisson-Model, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
glm_tenure <- glm(tenure ~ InternetService + Contract + PaymentMethod +
                    StreamingTV + StreamingMovies + SeniorCitizen +
                    Partner + MonthlyCharges + TotalCharges,
                  family = poisson(link = "log"), data = d.cleaned_telecom_customer_churn)

summary(glm_tenure)
```

## Key Findings

1.  **Contract Type**:
    -   Customers with a **One-year contract** are expected to have a tenure **31.5% longer** than those with a month-to-month contract.
    -   Customers with a **Two-year contract** have a **31.7% longer tenure**, making contract type a key predictor of retention.
2.  **Internet Service**:
    -   **Fiber optic users** have a slightly **higher tenure rate (+2.9%)** compared to DSL users.
    -   Customers with **No internet service** show a significant **decrease in tenure (-4.7%)**, suggesting bundled services may improve retention.
3.  **Payment Method**:
    -   Customers paying via **Mailed check** have significantly **shorter tenures (-27%)** compared to electronic payment users.
    -   **Credit card automatic payments** also show slightly shorter tenure (-20%), indicating that payment methods might influence retention.
4.  **Streaming Services**:
    -   **Streaming services (TV and Movies)** do not appear to significantly impact tenure, suggesting these features may not drive long-term customer retention.
5.  **Senior Citizen**:
    -   **Senior citizens** have a **7.4% longer tenure** than non-senior citizens, showing their loyalty to services.
6.  **Monthly and Total Charges**:
    -   **Monthly Charges** negatively impact tenure, with higher monthly charges leading to shorter tenures.
    -   **Total Charges** positively influence tenure, as expected for long-term customers.

## Evaluating the model:

**1. Goodness of Fit:**

-   **Residual Deviance**:
    -   The residual deviance is **43,105** on **7,029 degrees of freedom**.
    -   This ratio (Degrees of Freedom/ Residual Deviance) is approximately **6.13**, which is significantly greater than 1. This indicates **overdispersion** (variance \> mean), suggesting the Poisson model may not be a perfect fit.

**2. Significant Predictors:**

-   Predictors with extremely low p-values (p\<0.001p \< 0.001p\<0.001) include:
    -   **Contract type** (One-year and Two-year contracts)
    -   **Internet Service** (Fiber optic and No Internet Service)
    -   **Payment Method** (Mailed check and Credit card)
    -   **Senior Citizen**, **Partner**, **Monthly Charges**, and **Total Charges**
-   Non-significant predictors:
    -   **StreamingTV** and **StreamingMovies** were not significant, indicating they may not have a strong impact on tenure.

**3. Overdispersion Check:**

-   Overdispersion occurs when the variance of the response variable is greater than the mean.
    -   **Mean of Tenure**: \~32 months
    -   **Variance of Tenure**: Much larger than the mean, indicating overdispersion.
    -   This can be addressed by using:
        -   **Quasi-Poisson Regression**: Allows for overdispersion by adjusting the standard errors.

## Visualization.

**Boxplot for Contract**

```{r Poisson-Visualizations, echo=FALSE, results='asis', cache=TRUE}
# Boxplot for Contract
ggplot(d.cleaned_telecom_customer_churn, aes(x = Contract, y = tenure)) +
  geom_boxplot(fill = "lightblue", outlier.color = "red") +
  labs(title = "Tenure vs Contract Type", x = "Contract Type", y = "Tenure (Months)") +
  theme_minimal()
```

-   **Month-to-month contracts** show the shortest median tenure, with many outliers indicating customers leaving quickly.
-   **One-year** and **Two-year contracts** significantly increase tenure, with Two-year contracts having the longest median tenure.

Boxplot for Internet Service

```{r Poisson-Visualizations2, echo=FALSE, results='asis', cache=TRUE}
# Boxplot for Internet Service
ggplot(d.cleaned_telecom_customer_churn, aes(x = InternetService, y = tenure)) +
  geom_boxplot(fill = "lightgreen", outlier.color = "red") +
  labs(title = "Tenure vs Internet Service", x = "Internet Service Type", y = "Tenure (Months)") +
  theme_minimal()
```

-   Customers with **No internet service** tend to have shorter tenures.
-   **DSL** and **Fiber optic** services show similar median tenures, indicating these internet options retain customers for similar durations

Model with Quasi- Poisson Model with interaction Terms:

```{r Poisson-QuasiModel, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
glm_quasi <- glm(tenure ~ InternetService * Contract + PaymentMethod * Contract +
                   StreamingTV + StreamingMovies + SeniorCitizen + Partner +
                   MonthlyCharges + TotalCharges,
                 family = quasipoisson(link = "log"),
                 data = d.cleaned_telecom_customer_churn)
summary(glm_quasi)
```

**Model Performance**

1.  Dispersion Parameter:
    -   The dispersion parameter for the quasi-Poisson model is **3.99**, confirming overdispersion (variance \> mean).
    -   This indicates that the quasi-Poisson model is better suited for this dataset compared to the Poisson model.
2.  Residual Deviance:
    -   **Residual Deviance**: 31,910 (on 7,019 degrees of freedom), consistent with the Poisson model.
    -   This shows that the fit remains the same, but the quasi-Poisson model adjusts the standard errors for overdispersion, leading to more reliable p-values.

**Insights**

1.  Interaction Terms are Significant:
    -   Significant interaction effects for **Internet Service x Contract** and **Payment Method x Contract** highlight the importance of combining these factors to predict tenure effectively.
2.  Adjusted Standard Errors:
    -   Quasi-Poisson correction adjusts the standard errors, resulting in more reliable significance levels compared to the Poisson model.
3.  Non-Significant Predictors:
    -   **Streaming Services** and **Credit card (automatic)** are not significant, suggesting they may not strongly impact tenure.

### Business Insights and Recommendations

1.  **Encourage Long-Term Contracts**:
    -   Promote One-year and Two-year contracts, particularly for customers with **No Internet Service**, to boost retention.
2.  **Optimize Internet Service Offerings**:
    -   Address potential dissatisfaction among **Fiber optic** customers by enhancing service quality or offering competitive pricing.
    -   Bundling internet services with contracts can significantly improve tenure.
3.  **Reevaluate Payment Methods**:
    -   Reduce reliance on **Mailed checks**, as they correlate with shorter tenure.
    -   Incentivize automatic payments to retain customers longer.
4.  **Pricing Sensitivity**:
    -   Monitor **Monthly Charges** closely and consider pricing strategies or discounts to improve retention among price-sensitive customers.

Customer **tenure** is strongly influenced by contract type, internet service, and payment methods, with significant interaction effects between these factors. Long-term contracts and bundled internet services emerge as the most effective strategies to enhance customer retention.

\newpage

# GLM-Binomial (Logistic Regression) & GAM
*Sarp Koc took the lead on the GLM-Binomial and GAM models.*

## Context and Objective

The GLM-Binomial (Logistic Regression) and Generalized Additive Model (GAM) are used to predict customer churn and explore its underlying drivers. These models provide a foundation for identifying at-risk customers and evaluating key variables such as tenure, contract type, and payment methods. The analysis focuses on determining whether non-linear relationships exist in the data and comparing the performance of GLM and GAM in capturing these dynamics. By leveraging both linear and non-linear approaches, this section aims to enhance TeleConnect's retention strategies through actionable insights.

## Data Preparation and Visualization

First let's prepare the data for the visualization and modelling.

```{r GLM-Binomial-GAM-DataPreparation, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}

# Load required libraries
library(ggplot2)
library(dplyr)
library(mgcv)
library(pROC)
library(caret)

# Load the data
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)
data$Churn <- ifelse(data$Churn == "Yes", 1, 0)
# Ensure variables are factors
data$Contract <- as.factor(data$Contract)
data$PaymentMethod <- as.factor(data$PaymentMethod)
data$Churn <- as.factor(data$Churn)
data$TotalCharges <- log(data$TotalCharges)
data$MonthlyCharges <- log(data$MonthlyCharges)
data$InternetService <- as.factor(data$InternetService)

#str(data)
#colSums(is.na(data))
```

Let's plot some results to understand the data better.

## Plot: Tenure vs. TotalCharges by Churn

```{r GLM-Binomial-GAM-Visualizations, echo=FALSE, results='show', cache=TRUE}
# Ensure necessary libraries are installed
if(!require(ggplot2)) install.packages("ggplot2")

# Load the library
library(ggplot2)


ggplot(data, aes(x = tenure, y = TotalCharges, color = Churn)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Relationship Between Tenure and Total Charges by Churn",
    x = "Tenure (Months)",
    y = "Total Charges"
  ) +
  scale_color_manual(values = c("blue", "red"), labels = c("No Churn", "Churn")) +
  theme_minimal()
```

Higher tenure and total charges are associated with lower churn, reinforcing loyalty among long-term customers with higher spending.

## Plot: Tenure by Contract Type

```{r GLM-Binomial-GAM-Visualizations2, echo=FALSE, results='asis', cache=TRUE}

ggplot(data, aes(x = Contract, y = tenure, fill = Contract)) +
  geom_boxplot(alpha = 0.7) +
  labs(
    title = "Tenure by Contract Type",
    x = "Contract Type",
    y = "Tenure (Months)"
  ) +
  scale_fill_manual(values = c("blue", "orange", "purple")) +
  theme_minimal()
```

Longer contract durations (e.g., two years) correlate with higher customer retention, likely due to lower churn incentives.

## Plot: Churn by Payment Method

```{r GLM-Binomial-GAM-Visualizations3, echo=FALSE, results='asis', cache=TRUE}

ggplot(data, aes(x = PaymentMethod, fill = Churn)) +
  geom_bar(position = "fill", alpha = 0.8) +
  labs(
    title = "Proportion of Churn by Payment Method",
    x = "Payment Method",
    y = "Proportion"
  ) +
  scale_fill_manual(values = c("blue", "red"), labels = c("No Churn", "Churn")) +
  theme_minimal() +
  coord_flip()
```

Customers using electronic checks exhibit the highest churn proportion, suggesting potential dissatisfaction or difficulties with this payment method.

## Plot: Churn Proportion by Internet Service

```{r GLM-Binomial-GAM-Visualizations4, echo=FALSE, results='asis', cache=TRUE}
# Load necessary libraries
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(dplyr)) install.packages("dplyr")

library(ggplot2)
library(dplyr)

# Create a dataset for proportions and counts
proportion_df <- data %>%
  group_by(InternetService, Churn) %>%
  summarise(Count = n()) %>%
  mutate(Total = sum(Count),
         Proportion = Count / Total)

# Plot the bar chart with counts displayed
ggplot(proportion_df, aes(x = InternetService, y = Proportion, fill = as.factor(Churn))) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = Count), position = position_dodge(width = 0.9), vjust = -0.2, size = 3) +
  labs(
    title = "Proportion of Churn by Internet Service with Counts",
    x = "Internet Service Type",
    y = "Proportion",
    fill = "Churn"
  ) +
  scale_fill_manual(values = c("blue", "red"), labels = c("No", "Yes")) +
  theme_minimal()
```

Fiber optic users have significantly higher churn rates than DSL or users without internet service, indicating dissatisfaction with fiber optic services.

Let's create models of GLM and GAM to see if they catch these relations and how will they perform compared to each other.

## GLM

```{r GLM, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}

# Set 'No service' as the reference level for InternetService
data$InternetService <- relevel(data$InternetService, ref = "No")
# Confirm reference level
cat("Reference level for 'Contract':", levels(data$Contract)[1], "\n")
cat("Reference level for 'InternetService':", levels(data$InternetService)[1], "\n")
# Fit the GLM with interaction terms
glm_model_interaction <- glm(Churn ~ SeniorCitizen + Contract + PaymentMethod + 
                             tenure + TotalCharges + 
                             InternetService + OnlineSecurity + TechSupport + 
                             PaperlessBilling + MultipleLines, 
                             family = binomial, data = data)
# Check the summary
summary(glm_model_interaction)

```

So let's interpret the results:

Fiber optic users are more likely to churn, while DSL users are less likely to churn compared to no internet service customers.

Electronic check users are at higher risk of churn.

Customers with higher tenure and total charges are less likely to churn.

Longer contracts significantly reduce churn risk.

Senior citizens are more likely to churn

Electronic check users are more likely to churn.

Online security increases churn

Tech support significantly increases churn risk.

Longer contracts significantly reduce churn.

## Did the model overfit?

```{r GLM-Overfitting, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(pROC)
# Split data into training and test sets
set.seed(123)
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

# Predict on training and test data
train_preds <- predict(glm_model_interaction, train_data, type = "response")
test_preds <- predict(glm_model_interaction, test_data, type = "response")

# Calculate performance metrics
library(pROC)
train_auc <- roc(train_data$Churn, train_preds)$auc
test_auc <- roc(test_data$Churn, test_preds)$auc

cat("Train AUC:", train_auc, "\n")
cat("Test AUC:", test_auc, "\n")
```

The Train AUC (0.847) and Test AUC (0.854) are very close, indicating that the model performs consistently on both the training and test datasets. This suggests good generalization without overfitting to the training data.

```{r GLM-CrossValidation, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(caret)

# Set up cross-validation
control <- trainControl(method = "cv", number = 10) # 10-fold cross-validation

# Train the model
cv_model <- caret::train(Churn ~ SeniorCitizen + Contract + PaymentMethod + 
                  tenure + MonthlyCharges + TotalCharges + 
                  InternetService + InternetService:MonthlyCharges, 
                  data = data, method = "glm", family = "binomial", trControl = control)

# Check cross-validation results
print(cv_model)
```

If there were significant overfitting, we would expect a much larger difference between training and cross-validation accuracy. So it seems there is no overfit

## GAM with non-linear patterns

```{r GAM-WithNonLinearPatterns, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(mgcv)
data$TotalCharges_centered <- data$TotalCharges- mean(data$TotalCharges)
cat("Reference level for 'Contract':", levels(data$Contract)[1], "\n")
cat("Reference level for 'InternetService':", levels(data$InternetService)[1], "\n")
gam_smoothing_spline <- gam(Churn ~ SeniorCitizen + Contract + PaymentMethod + 
                             s(tenure) + s(TotalCharges_centered) + 
                             InternetService + OnlineSecurity + TechSupport + 
                             PaperlessBilling + MultipleLines, 
                             family = binomial, data = data, select=TRUE)
summary(gam_smoothing_spline)
```

edf shows that mooth terms for tenure and TotalCharges_centered are highly significant (p \< 0.001), indicating non-linear effects. Relationship is non-linear but not overly complex.

## ROC Comparison

Let's compare these models.

```{r ROC-Comparison, echo=FALSE, results='asis', cache=TRUE}

library(pROC)

# Predictions for glm_model
pred1 <- predict(glm_model_interaction, type = "response")
roc1 <- roc(data$Churn, pred1)

pred2 <- predict(gam_smoothing_spline, type = "response")
roc2 <- roc(data$Churn, pred2)

# Plot ROC curves
plot(roc1, col = "blue", lwd = 2, main = "ROC Curve Comparison")
lines(roc2, col = "red", lwd = 2)
legend("bottomright", legend = c("glm_model_interaction", "gam_smoothing_spline"),
       col = c("blue", "red"), lwd = 2)

auc1 <- auc(roc1)
auc2 <- auc(roc2)
cat("AUC of Model glm_model_interaction:", auc1, "\n")
cat("AUC of Model gam_smoothing_spline:", auc2, "\n")
```

Both models perform similarly in terms of AUC, but GAM slightly edges out GLM in predictive performance.

```{r Anova-Comparison, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
anova(glm_model_interaction, gam_smoothing_spline, test = "Chisq")
```

GAM explains more variance than GLM.

```{r AIC-Comparison, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
AIC(glm_model_interaction, gam_smoothing_spline)
```

GAM has a slightly lower AIC (5809.896) than GLM (5828.536), further supporting that GAM provides a better fit to the data.

## Conclusion

Both models performed well in predicting churn, with GAM offering additional flexibility to capture non-linear relationships. While the non-linearity in some variables, like tenure, was not significant, GAM successfully identified a meaningful non-linear relationship with TotalCharges. The results highlight that customers are more likely to churn if they use fiber optic internet, pay via electronic checks, lack online security or tech support, or are senior citizens. Conversely, churn risk is lower for DSL users, those with higher tenure and total charges, and customers on longer-term contracts.

# Neural Network (NN)
*Kenny Trinh took the lead on the Neural Network model.*

## Context and Objective

TeleConnect aims to leverage Neural Network (NN) models to enhance customer retention by identifying churn-prone customers with high accuracy. The NN model was trained and tested to predict churn outcomes using customer behavioral and demographic data. This analysis focuses on understanding key metrics like sensitivity, specificity, and precision to refine TeleConnect's retention strategies.

## Data Preparation for Neural Network

```{r NN-DataPreparation, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Load necessary libraries
library(caret)
library(nnet)
library(pROC)
library(ggplot2)
library(dplyr)
library(RSNNS)

# Load the cleaned dataset
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

# Convert categorical variables to factors
data$Churn <- as.factor(ifelse(data$Churn == "Yes", 1, 0))

# Check and convert other categorical variables
data$gender <- as.factor(data$gender)
data$Partner <- as.factor(data$Partner)
data$PhoneService <- as.factor(data$PhoneService)
data$MultipleLines <- as.factor(data$MultipleLines)
data$InternetService <- as.factor(data$InternetService)
data$OnlineSecurity <- as.factor(data$OnlineSecurity)
data$OnlineBackup <- as.factor(data$OnlineBackup)
data$DeviceProtection <- as.factor(data$DeviceProtection)
data$TechSupport <- as.factor(data$TechSupport)
data$StreamingTV <- as.factor(data$StreamingTV)
data$StreamingMovies <- as.factor(data$StreamingMovies)
data$Contract <- as.factor(data$Contract)
data$PaperlessBilling <- as.factor(data$PaperlessBilling)
data$PaymentMethod <- as.factor(data$PaymentMethod)

# Convert TotalCharges to numeric (handle any non-numeric issues)
data$TotalCharges <- as.numeric(as.character(data$TotalCharges))

data <- na.omit(data)  # Remove rows with missing values

# Split the data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(data$Churn, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Preprocess the data: scale numeric columns
preProc <- preProcess(trainData[, c("tenure", "MonthlyCharges",
                                    "TotalCharges")],
                      method = c("center", "scale"))
trainData[, c("tenure", "MonthlyCharges", "TotalCharges")] <- predict(
  preProc, trainData[, c("tenure", "MonthlyCharges", "TotalCharges")])
testData[, c("tenure", "MonthlyCharges", "TotalCharges")] <- predict(
  preProc, testData[, c("tenure", "MonthlyCharges", "TotalCharges")])
```

The data is prepared for the Neural Network model by splitting it into training and testing sets and scaling numerical features for consistent input ranges. This step ensures the NN model can effectively learn patterns.



## Training the Neural Network

### Exploring different Neural Network Configurations

| **Config.** | **Size** | **Decay** | **Accuracy** | **Sensitivity** | **Specificity** | **Balanced Accuracy** | **Kappa** |
|--------|--------|--------|--------|--------|--------|--------|--------|
| **1** | **2** | **0.04** | **81.38%** | **91.10%** | **54.42%** | **72.76%** | **0.4879** |
| **2** | 1 | 0.04 | 80.88% | 90.04% | 55.50% | 72.77% | 0.4813 |
| **3** | 7 | 0.04 | 79.74% | 88.30% | 56.03% | 72.17% | 0.4603 |
| **4** | 5 | 0.04 | 80.60% | 90.14% | 54.16% | 72.15% | 0.4707 |

: Neural Network Configurations and Performance Metrics

**Comparison:**

- Configuration 1:
  - Best overall.
  - Has the best balance across all metrics.
- Configuration 2:
  - High simplicity.
  - Simple with strong performance.
- Configuration 3:
  - Best trade-off.
  - Balanced with slightly higher specificity.
- Configuration 4:
  - Moderate complexity.
  - Great balance.
  
Key Insights from Neural Network Configurations:

1.  **Best Overall Model**:
    -   **Size = 2, Decay = 0.04** delivers the best trade-off between sensitivity (91.10%) and specificity (54.42%), making it ideal for practical deployment.
    -   Balances customer identification (high sensitivity) with resource management (moderate specificity).
2.  **Simpler Options**:
    -   **Size = 1, Decay = 0.04** is a lightweight model for quicker predictions while maintaining strong accuracy.
3.  **Focused Trade-offs**:
    -   **Size = 7, Decay = 0.04** provides higher specificity for targeted retention campaigns.
    -   Suitable for businesses with resource constraints aiming to minimize false positives.

### Training with an optimal configuration

```{r NN-TrainNN, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Train the Neural Network
nn_model <- nnet(
  Churn ~ ., data = trainData,
  size = 2, decay = 0.04, maxit = 300, trace = FALSE
)

# Predict on the test set
nn_prob <- predict(nn_model, testData, type = "raw")
```

A Neural Network is trained with 2 hidden neurons (size) and a regularization parameter (decay) of 0.04. The decay helps prevent overfitting by penalizing large weights. Predictions are then made on the test set to evaluate performance.

## Evaluating the Model

```{r NN-EvaluateModel, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}

# Evaluate the model
# Confusion matrix
nn_pred <- as.factor(ifelse(nn_prob > 0.5, 1, 0))
conf_matrix <- confusionMatrix(nn_pred, testData$Churn)
print(conf_matrix)

# Compute and plot the ROC curve
nn_roc <- roc(testData$Churn, as.numeric(nn_prob), 
              levels = rev(levels(testData$Churn)))
plot(nn_roc, col = "blue", main = "Neural Network ROC Curve")
auc_value <- auc(nn_roc)
cat("Neural Network AUC:", auc_value, "\n")
```

The Neural Network's performance is evaluated using a confusion matrix and ROC curve. The Area Under the Curve (AUC) quantifies how well the model distinguishes between churners and non-churners.

## Threshold Adjustment

```{r NN-ThresholdAdjustment, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Adjust the threshold to improve specificity
threshold <- 0.54  # Adjust to a higher value to improve specificity
nn_pred_adjusted <- as.factor(ifelse(nn_prob > threshold, 1, 0))

# Generate a confusion matrix for the adjusted threshold
conf_matrix_adjusted <- confusionMatrix(nn_pred_adjusted, testData$Churn)
print(conf_matrix_adjusted)

# Recalculate and plot the ROC curve for the adjusted predictions
nn_roc_adjusted <- roc(testData$Churn, as.numeric(nn_prob), 
                       levels = rev(levels(testData$Churn)))
plot(nn_roc_adjusted, col = "red", 
     main = "Neural Network ROC Curve (Adjusted Threshold)")
auc_value_adjusted <- auc(nn_roc_adjusted)
cat("Neural Network AUC (Adjusted Threshold):", auc_value_adjusted, "\n")
```

The decision threshold for classifying churners is adjusted to 0.54, focusing on improving specificity (reducing false positives) while maintaining acceptable sensitivity. The updated confusion matrix is then evaluated.

## Final NN Model Performance (Resampling with Weight Ratio)

```{r NN-ResamplingWithWeightRatio, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(nnet)
library(caret)
library(pROC)

# Assign weights to each class (focus on minority class)
weights <- ifelse(trainData$Churn == 1, 1.5, 1)

# Perform weighted resampling
trainData_resampled <- trainData[rep(1:nrow(trainData), times = weights), ]

# Train a Neural Network model on the resampled data
nn_model_resampled <- nnet::nnet(
  Churn ~ ., data = trainData_resampled,
  size = 2, decay = 0.04, maxit = 300, trace = FALSE
)

# Predict on the test set
nn_prob_resampled <- predict(nn_model_resampled, testData, type = "raw")

# Evaluate model performance
nn_pred_resampled <- as.factor(ifelse(nn_prob_resampled > 0.5, 1, 0))
conf_matrix_resampled <- caret::confusionMatrix(nn_pred_resampled, testData$Churn)
print(conf_matrix_resampled)

# Plot the ROC Curve for the resampled model
nn_roc_resampled <- pROC::roc(testData$Churn, as.numeric(nn_prob_resampled), levels = rev(levels(testData$Churn)))
plot(nn_roc_resampled, col = "blue",
     main = "Neural Network ROC Curve (Resampled Data)")

auc_resampled <- auc(nn_roc_resampled)
cat("Neural Network AUC (Resampled Data):", auc_resampled, "\n")
```

## Neural Network with RSNNS

```{r NN-RSNNS, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Convert categorical variables to dummy variables
train_data_numeric <- model.matrix(~ . - 1, data = trainData[, -which(names(trainData) == "Churn")])  # Remove intercept
test_data_numeric <- model.matrix(~ . - 1, data = testData[, -which(names(testData) == "Churn")])    # Remove intercept

# Ensure the labels are numeric binary
train_labels <- as.numeric(trainData$Churn) - 1
test_labels <- as.numeric(testData$Churn) - 1


# Train the neural network with 2 hidden layers and adjusted neurons
nn_model_adjusted <- mlp(
  x = train_data_numeric, y = train_labels,
  size = c(10, 5),   # Two hidden layers: 10 neurons in the first, 5 in the second
  maxit = 300,
  learnFuncParams = c(0.1),  # Learning rate
  hiddenActFunc = "Act_Logistic"  # Default logistic activation function
)

# Predict on the test data
nn_pred_adjusted <- predict(nn_model_adjusted, test_data_numeric)
nn_pred_class <- as.factor(ifelse(nn_pred_adjusted > 0.5, 1, 0))

# Evaluate the model
conf_matrix_adjusted <- confusionMatrix(nn_pred_class, as.factor(test_labels))
print(conf_matrix_adjusted)

# Extract values from the confusion matrix
confusion_matrix <- table(test_labels, nn_pred_class)

TP <- confusion_matrix[2, 2]  # True Positives
FN <- confusion_matrix[2, 1]  # False Negatives
FP <- confusion_matrix[1, 2]  # False Positives
TN <- confusion_matrix[1, 1]  # True Negatives

# Calculate the metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)  # Recall
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)

# Print the metrics
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Sensitivity (Recall):", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
```

## Neural Network with RSNNS: Trying Alternative Activation Functions

```{r NN-Conclusion, echo=FALSE, results='asis', message=FALSE, warning=FALSE, cache=TRUE}
# Convert categorical variables to dummy variables
train_data_numeric <- model.matrix(~ . - 1, data = trainData[, -which(names(trainData) == "Churn")])  # Remove intercept
test_data_numeric <- model.matrix(~ . - 1, data = testData[, -which(names(trainData) == "Churn")])    # Remove intercept

# Ensure the labels are numeric binary
train_labels <- as.numeric(trainData$Churn) - 1
test_labels <- as.numeric(testData$Churn) - 1

# Experiment with Act_TanH activation function
nn_model_tanh <- mlp(
  x = train_data_numeric, 
  y = train_labels,
  size = c(10, 5),   # Two hidden layers: 10 neurons in the first, 5 in the second
  maxit = 300,
  learnFuncParams = c(0.1),  # Learning rate
  hiddenActFunc = "Act_TanH"  # Tanh activation function
)

# Predict and evaluate for Act_TanH
nn_pred_tanh <- predict(nn_model_tanh, test_data_numeric)
nn_pred_class_tanh <- as.factor(ifelse(nn_pred_tanh > 0.5, 1, 0))
conf_matrix_tanh <- confusionMatrix(nn_pred_class_tanh, as.factor(test_labels))
print("Confusion Matrix (Act_TanH):")
print(conf_matrix_tanh)

# Experiment with Act_ReLU activation function
nn_model_relu <- mlp(
  x = train_data_numeric, 
  y = train_labels,
  size = c(10, 5),   # Two hidden layers: 10 neurons in the first, 5 in the second
  maxit = 300,
  learnFuncParams = c(0.1),  # Learning rate
  hiddenActFunc = "Act_ReLU"  # ReLU activation function
)

# Predict and evaluate for Act_ReLU
nn_pred_relu <- predict(nn_model_relu, test_data_numeric)
nn_pred_class_relu <- as.factor(ifelse(nn_pred_relu > 0.5, 1, 0))
conf_matrix_relu <- confusionMatrix(nn_pred_class_relu, as.factor(test_labels))
print("Confusion Matrix (Act_ReLU):")
print(conf_matrix_relu)


# Extract values from the confusion matrix for Act_TanH
confusion_matrix_tanh <- table(test_labels, nn_pred_class_tanh)
TP_tanh <- confusion_matrix_tanh[2, 2]  # True Positives
FN_tanh <- confusion_matrix_tanh[2, 1]  # False Negatives
FP_tanh <- confusion_matrix_tanh[1, 2]  # False Positives
TN_tanh <- confusion_matrix_tanh[1, 1]  # True Negatives

# Calculate metrics for Act_TanH
accuracy_tanh <- (TP_tanh + TN_tanh) / sum(confusion_matrix_tanh)
sensitivity_tanh <- TP_tanh / (TP_tanh + FN_tanh)  # Recall
specificity_tanh <- TN_tanh / (TN_tanh + FP_tanh)
precision_tanh <- TP_tanh / (TP_tanh + FP_tanh)

# Print metrics for Act_TanH
cat("Metrics for Act_TanH:\n")
cat("Accuracy:", round(accuracy_tanh, 4), "\n")
cat("Sensitivity (Recall):", round(sensitivity_tanh, 4), "\n")
cat("Specificity:", round(specificity_tanh, 4), "\n")
cat("Precision:", round(precision_tanh, 4), "\n\n")

# Extract values from the confusion matrix for Act_ReLU
confusion_matrix_relu <- table(test_labels, nn_pred_class_relu)
TP_relu <- confusion_matrix_relu[2, 2]  # True Positives
FN_relu <- confusion_matrix_relu[2, 1]  # False Negatives
FP_relu <- confusion_matrix_relu[1, 2]  # False Positives
TN_relu <- confusion_matrix_relu[1, 1]  # True Negatives

# Calculate metrics for Act_ReLU
accuracy_relu <- (TP_relu + TN_relu) / sum(confusion_matrix_relu)
sensitivity_relu <- TP_relu / (TP_relu + FN_relu)  # Recall
specificity_relu <- TN_relu / (TN_relu + FP_relu)
precision_relu <- TP_relu / (TP_relu + FP_relu)

# Print metrics for Act_ReLU
cat("Metrics for Act_ReLU:\n")
cat("Accuracy:", round(accuracy_relu, 4), "\n")
cat("Sensitivity (Recall):", round(sensitivity_relu, 4), "\n")
cat("Specificity:", round(specificity_relu, 4), "\n")
cat("Precision:", round(precision_relu, 4), "\n")

```

## Neural Network with Regularization and Adjusted Architecture

```{r NN-RegularizationAndAdjustedArchitecture, echo=FALSE, results='asis', message=FALSE, warning=FALSE, cache=TRUE}

# Convert categorical variables to dummy variables
train_data_numeric <- model.matrix(~ . - 1, data = trainData[, -which(names(trainData) == "Churn")])  # Remove intercept
test_data_numeric <- model.matrix(~ . - 1, data = testData[, -which(names(testData) == "Churn")])    # Remove intercept

# Ensure the labels are numeric binary
train_labels <- as.numeric(trainData$Churn) - 1
test_labels <- as.numeric(testData$Churn) - 1

# Train the neural network with 3 hidden layers, regularization, and adjusted neurons
nn_model_regularized <- mlp(
  x = train_data_numeric, 
  y = train_labels,
  size = c(15, 10, 5),       # Three hidden layers: 15, 10, and 5 neurons
  maxit = 300,
  learnFuncParams = c(0.1, 0.001),  # Learning rate and weight decay (L2 regularization)
  hiddenActFunc = "Act_Logistic"    # Logistic activation function
)

# Predict and evaluate
nn_pred_regularized <- predict(nn_model_regularized, test_data_numeric)
nn_pred_class_regularized <- as.factor(ifelse(nn_pred_regularized > 0.5, 1, 0))
conf_matrix_regularized <- confusionMatrix(nn_pred_class_regularized, as.factor(test_labels))

# Extract metrics
confusion_matrix_regularized <- table(test_labels, nn_pred_class_regularized)
TP <- confusion_matrix_regularized[2, 2]
FN <- confusion_matrix_regularized[2, 1]
FP <- confusion_matrix_regularized[1, 2]
TN <- confusion_matrix_regularized[1, 1]

accuracy <- (TP + TN) / sum(confusion_matrix_regularized)
sensitivity <- TP / (TP + FN)  # Recall
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)

# Print results
cat("Metrics for Regularized Neural Network with Adjusted Architecture:\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Sensitivity (Recall):", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
```

## Neural Network Experimentation with Different Architectures

```{r NN-ExperimentationWithDifferentArchitectures, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
library(RSNNS)
library(ggplot2)

# Define a list of architectures to test
architectures <- list(
  c(20, 10, 5),
  c(10, 5),
  c(25, 15, 10)
)

# Initialize a data frame to store results
architecture_results <- data.frame(
  Architecture = character(),
  Accuracy = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  Precision = numeric()
)

# Loop through each architecture
for (arch in architectures) {
  cat("Testing architecture:", paste(arch, collapse = "-"), "\n")
  
  # Train the model with the given architecture
  nn_model <- RSNNS::mlp(
    x = train_data_numeric, 
    y = train_labels,
    size = arch,  # Set architecture
    maxit = 300,
    learnFuncParams = c(0.1, 0.001),  # Learning rate and L2 regularization
    hiddenActFunc = "Act_Logistic"  # Activation function
  )
  
  # Predict on the test data
  nn_pred <- predict(nn_model, test_data_numeric)
  nn_pred_class <- as.factor(ifelse(nn_pred > 0.5, 1, 0))
  
  # Confusion matrix and metrics
  confusion_matrix <- table(test_labels, nn_pred_class)
  TP <- confusion_matrix[2, 2]
  FN <- confusion_matrix[2, 1]
  FP <- confusion_matrix[1, 2]
  TN <- confusion_matrix[1, 1]
  
  accuracy <- (TP + TN) / sum(confusion_matrix)
  sensitivity <- TP / (TP + FN)  # Recall
  specificity <- TN / (TN + FP)
  precision <- TP / (TP + FP)
  
  # Store results
  architecture_results <- rbind(
    architecture_results,
    data.frame(
      Architecture = paste(arch, collapse = "-"),
      Accuracy = round(accuracy, 4),
      Sensitivity = round(sensitivity, 4),
      Specificity = round(specificity, 4),
      Precision = round(precision, 4)
    )
  )
}

# Print the results
print(architecture_results)

# Visualize the results
ggplot2::ggplot(architecture_results, aes(x = Architecture)) +
  geom_bar(aes(y = Accuracy), stat = "identity", fill = "steelblue") +
  geom_point(aes(y = Sensitivity), color = "red", size = 3) +
  geom_point(aes(y = Specificity), color = "green", size = 3) +
  geom_point(aes(y = Precision), color = "purple", size = 3) +
  labs(title = "Performance Metrics by Architecture", x = "Architecture", y = "Metrics") +
  theme_minimal()
```

```{r NN-AddingDropoutRegularization, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
nn_model_dropout <- mlp(
  x = train_data_numeric, 
  y = train_labels,
  size = c(20, 10, 5),  # Best architecture identified
  maxit = 300,
  learnFuncParams = c(0.05, 0.001),  # Best learning rate and regularization
  hiddenActFunc = "Act_Logistic",  # Logistic activation
  dropout = c(0.2, 0.2, 0.2)  # Dropout rates for each hidden layer
)

# Predict and evaluate
nn_pred_dropout <- predict(nn_model_dropout, test_data_numeric)
nn_pred_class_dropout <- as.factor(ifelse(nn_pred_dropout > 0.5, 1, 0))
conf_matrix_dropout <- confusionMatrix(nn_pred_class_dropout, as.factor(test_labels))

# Extract metrics for comparison
confusion_matrix_dropout <- table(test_labels, nn_pred_class_dropout)
TP <- confusion_matrix_dropout[2, 2]
FN <- confusion_matrix_dropout[2, 1]
FP <- confusion_matrix_dropout[1, 2]
TN <- confusion_matrix_dropout[1, 1]

accuracy_dropout <- (TP + TN) / sum(confusion_matrix_dropout)
sensitivity_dropout <- TP / (TP + FN)
specificity_dropout <- TN / (TN + FP)
precision_dropout <- TP / (TP + FP)

# Print results
cat("Metrics for Neural Network with Dropout Regularization:\n")
cat("Accuracy:", round(accuracy_dropout, 4), "\n")
cat("Sensitivity (Recall):", round(sensitivity_dropout, 4), "\n")
cat("Specificity:", round(specificity_dropout, 4), "\n")
cat("Precision:", round(precision_dropout, 4), "\n")


# Adjust the decision threshold
threshold <- 0.4  # Example threshold
nn_pred_class_threshold <- as.factor(ifelse(nn_pred_dropout > threshold, 1, 0))

# Evaluate with adjusted threshold
conf_matrix_threshold <- confusionMatrix(nn_pred_class_threshold, as.factor(test_labels))
confusion_matrix_threshold <- table(test_labels, nn_pred_class_threshold)
TP_thresh <- confusion_matrix_threshold[2, 2]
FN_thresh <- confusion_matrix_threshold[2, 1]
FP_thresh <- confusion_matrix_threshold[1, 2]
TN_thresh <- confusion_matrix_threshold[1, 1]

accuracy_thresh <- (TP_thresh + TN_thresh) / sum(confusion_matrix_threshold)
sensitivity_thresh <- TP_thresh / (TP_thresh + FN_thresh)
specificity_thresh <- TN_thresh / (TN_thresh + FP_thresh)
precision_thresh <- TP_thresh / (TP_thresh + FP_thresh)

# Print adjusted metrics
cat("Metrics with Adjusted Threshold (", threshold, "):\n", sep = "")
cat("Accuracy:", round(accuracy_thresh, 4), "\n")
cat("Sensitivity (Recall):", round(sensitivity_thresh, 4), "\n")
cat("Specificity:", round(specificity_thresh, 4), "\n")
cat("Precision:", round(precision_thresh, 4), "\n")
```

## Threshold Analysis and Visualization

```{r NN-ThresholdAnalysis, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Define thresholds to test
thresholds <- seq(0.1, 0.9, by = 0.05)

# Initialize a data frame to store results
threshold_results <- data.frame(
  Threshold = numeric(),
  Accuracy = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  Precision = numeric()
)

# Loop through thresholds
for (thresh in thresholds) {
  # Apply the threshold to predictions
  nn_pred_class_thresh <- as.factor(ifelse(nn_pred_dropout > thresh, 1, 0))
  
  # Confusion matrix and metrics
  confusion_matrix_thresh <- table(test_labels, nn_pred_class_thresh)
  TP_thresh <- confusion_matrix_thresh[2, 2]
  FN_thresh <- confusion_matrix_thresh[2, 1]
  FP_thresh <- confusion_matrix_thresh[1, 2]
  TN_thresh <- confusion_matrix_thresh[1, 1]
  
  # Calculate metrics
  accuracy_thresh <- (TP_thresh + TN_thresh) / sum(confusion_matrix_thresh)
  sensitivity_thresh <- TP_thresh / (TP_thresh + FN_thresh)
  specificity_thresh <- TN_thresh / (TN_thresh + FP_thresh)
  precision_thresh <- TP_thresh / (TP_thresh + FP_thresh)
  
  # Add results to the data frame
  threshold_results <- rbind(
    threshold_results,
    data.frame(
      Threshold = thresh,
      Accuracy = round(accuracy_thresh, 4),
      Sensitivity = round(sensitivity_thresh, 4),
      Specificity = round(specificity_thresh, 4),
      Precision = round(precision_thresh, 4)
    )
  )
}

# Print the results
print(threshold_results)

# Visualize the results
ggplot(threshold_results, aes(x = Threshold)) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  geom_line(aes(y = Sensitivity, color = "Sensitivity")) +
  geom_line(aes(y = Specificity, color = "Specificity")) +
  geom_line(aes(y = Precision, color = "Precision")) +
  labs(title = "Performance Metrics Across Thresholds",
       x = "Threshold",
       y = "Metrics",
       color = "Metrics") +
  theme_minimal()

```

## Evaluation Metrics

1. **Confusion Matrix Results**:
    - **Accuracy**: ~81% (overall correct predictions).
    - **Sensitivity**: ~91% (ability to correctly identify churners).
    - **Specificity**: ~54% (ability to identify non-churners accurately).
2. **Threshold Adjustment**:
    - Threshold **0.40** optimized for balancing sensitivity (61%) and specificity (85%).
    - Threshold **0.50** prioritizes fewer false positives but reduces sensitivity (50.67%).
3. **ROC Curve and AUC**:
    - **AUC**: 0.8446, indicating excellent model performance in distinguishing between churners and non-churners.
4. **Precision**:
    - ~60%, ensuring the majority of predicted churners are valid.


## Business Insights from Neural Network Results

1. **Sensitivity Priority**:
    - High sensitivity aligns with TeleConnect’s objective to minimize customer churn by identifying most at-risk customers.
    - Enables proactive retention strategies to prevent revenue loss.
2. **Specificity Trade-off**:
    - Moderate specificity indicates some false positives, acceptable when prioritizing churn reduction over cost minimization.
    - TeleConnect can address this by segmenting high-risk customers for targeted campaigns.
3. **Threshold Customization**:
    - **Lower Threshold (e.g., 0.40)**: Broader customer outreach, ideal for high churn rates and flexible budgets.
    - **Higher Threshold (e.g., 0.50)**: Focused retention for high-value customers, reducing unnecessary interventions.

## Strategic Recommendations

1. **Retention Campaigns**:
    - **Targeted Outreach**: Focus on high-risk churners identified by NN with thresholds of 0.40–0.45.
    - **Proactive Offers**: Use predictive insights to design personalized offers (e.g., discounts, loyalty perks).
2. **Threshold Optimization**:
    - Adjust thresholds dynamically based on customer segments and campaign costs.
    - Example: Use a lower threshold for new customers (early churn risk) and a higher one for long-tenure, high-value customers.
3. **Model Deployment**:
    - Implement the **Size = 2, Decay = 0.04** configuration for its balanced performance.
    - Integrate real-time predictions into CRM systems to support customer retention teams.
4. **Future Enhancements**:
    - Experiment with ensemble models combining NN with other techniques (e.g., SVM, GAM) to improve overall accuracy and specificity.
    - Perform feature importance analysis to prioritize impactful variables like tenure, contract type, and internet service in model training.


The Neural Network model demonstrates robust capabilities in identifying churn-prone customers, aligning with TeleConnect’s goal to enhance customer retention. By fine-tuning thresholds and leveraging predictive insights, TeleConnect can effectively reduce churn rates, increase customer lifetime value, and optimize retention budgets. This model serves as a cornerstone for a data-driven retention strategy that balances sensitivity, specificity, and business objectives.



# Support Vector Machine (SVM)
*Kenny Trinh took the lead on the Support Vector Machine model.*

## Context and Objective
The Support Vector Machine (SVM) model is employed to predict customer churn by efficiently separating churners and non-churners, even in complex and non-linear data scenarios. This analysis leverages SVM to classify customers with high accuracy, ensuring robust segmentation into low, medium, and high-risk churn groups. The objective is to provide actionable insights for targeted retention strategies while identifying critical factors influencing churn probability.

## Model Performance Metrics

The SVM model performance evaluation yields the following results:

1. **Accuracy**: **80.24%**
    - The proportion of correctly predicted churn and non-churn cases out of all predictions.
2. **Precision (Positive Predictive Value)**: **82.53%**
    - The proportion of churn predictions that are actually correct.
3. **Recall (Sensitivity)**: **92.75%**
    - The ability of the model to correctly identify customers who churn.
4. **F1-Score**: **87.34%**
    - A balanced metric combining precision and recall, ensuring both false positives and false negatives are minimized.

**Interpretation**:

- The model effectively identifies churners (high recall) while maintaining a good balance with precision.
- The F1-Score indicates strong performance for churn prediction, balancing false positives and false negatives.

## Churn Probability Prediction

Using the trained SVM model, churn probabilities were calculated for each customer.

**Steps**:

1. The SVM model was tuned using radial kernel with the best parameters: `Cost = 10`, `Gamma = 0.1`.
2. Predictions were generated, including probabilities for both classes (`Yes` for churn, `No` for non-churn).

```{r SVM-ChurnProbabilityPrediction, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Load necessary libraries
library(e1071)
library(caret)
library(ggplot2)
library(pROC)
library(dplyr)

# Load the cleaned dataset
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

# Check for missing values
sum(is.na(data))

# Convert target variable to a factor
data$Churn <- as.factor(data$Churn)

# Split the dataset into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$Churn, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# ------------------ INITIAL SVM MODEL ------------------
# Train the SVM model
svm_model <- svm(Churn ~ ., data = trainData, kernel = "radial", cost = 1, 
                 gamma = 0.1)

# Make predictions on the test data
predictions <- predict(svm_model, testData)

# Evaluate the initial model
cat("\nConfusion Matrix for Initial SVM Model:\n")
confusionMatrix(predictions, testData$Churn)

# Paths to save tuned model and parameters
tuned_model_path <- "output/svm_tuned_model.rds"
tuned_params_path <- "output/svm_tuned_parameters.rds"

# ------------------ TUNED SVM MODEL ------------------
# Tune the SVM model for optimal hyperparameters or load if already saved
set.seed(123)
if (!file.exists(tuned_model_path) || !file.exists(tuned_params_path)) {
  cat("\nTuning SVM Model...\n")
  tuned_parameters <- tune(svm, Churn ~ ., data = trainData,
                           ranges = list(cost = c(0.1, 1, 10), 
                                         gamma = c(0.01, 0.1, 1)),
                           probability = TRUE)
  best_model <- tuned_parameters$best.model
  saveRDS(best_model, tuned_model_path)
  saveRDS(tuned_parameters, tuned_params_path)
} else {
  cat("\nLoading Tuned SVM Model...\n")
  best_model <- readRDS(tuned_model_path)
  tuned_parameters <- readRDS(tuned_params_path)
}

# Make predictions with the best model
best_predictions <- predict(best_model, testData, probability = TRUE)

# Evaluate the best model
cat("\nConfusion Matrix for Tuned SVM Model:\n")
conf_matrix <- confusionMatrix(best_predictions, testData$Churn)
print(conf_matrix)

# Extract probabilities
probabilities <- attr(predict(best_model, testData, probability = TRUE), "probabilities")

# ------------------ BEST PARAMETERS ------------------
# Check the best parameters chosen during tuning
cat("\nBest Parameters Chosen During Tuning:\n")
print(tuned_parameters$best.parameters)

# ------------------ PROBABILITIES ------------------
# Extract probabilities
probabilities <- attr(best_predictions, "probabilities")
cat("\nSample of Predicted Probabilities:\n")
print(head(probabilities))  # Display a sample of probabilities
```

### Insights

**Confusion Matrix for Initial SVM Model:** The initial model achieved good predictive performance, with a high count of true positives (churners correctly identified) and true negatives (non-churners correctly identified). However, some false negatives indicate a need for improved recall, which was addressed through hyperparameter tuning.

**Confusion Matrix for Tuned SVM Model:** After tuning, the model showed improvements in recall and precision, with reduced false negatives. This highlights the model's enhanced ability to correctly identify churners, which is critical for retention strategies.

**Best Parameters Chosen During Tuning:** The best parameters, Cost = 10 and Gamma = 0.01, were selected during the tuning process. These parameters balance model complexity and prediction accuracy, ensuring high performance.

**Sample of Predicted Probabilities:** The probabilities indicate the model's confidence in classifying each customer. For example, a probability of 38.59% churn suggests medium churn risk, while a probability of 18.52% churn indicates low churn risk. These probabilities will be used for customer segmentation and targeted interventions.

## Evaluate additional metrics for the tuned model

```{r SVM-AdditionalMetrics, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Ensure the predicted and actual values are factors
best_predictions <- as.factor(best_predictions)
testData$Churn <- as.factor(testData$Churn)

# Generate the confusion matrix using caret explicitly
conf_matrix <- caret::confusionMatrix(data = best_predictions,
                                      reference = testData$Churn)

# Extract Accuracy
accuracy <- conf_matrix$overall["Accuracy"]
cat("Accuracy:", accuracy, "\n")

# Extract Precision, Recall, and F1-Score
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")

# ---Generate the ROC curve and calculate AUC---
# Get decision values (raw scores) for predictions
svm_decision_values <- predict(best_model, testData, decision.values = TRUE)

# Extract decision values for the ROC curve
decision_values <- as.numeric(attr(svm_decision_values, "decision.values"))

# Recalculate the ROC curve
roc_curve <- roc(response = testData$Churn,
                 predictor = decision_values,
                 levels = rev(levels(testData$Churn)))

# Plot the corrected ROC curve
plot(roc_curve, col = "blue", main = "Corrected ROC Curve for Tuned SVM Model", lwd = 2)
auc_value <- auc(roc_curve)
cat("Corrected AUC:", auc_value, "\n")
```

## Customer Segmentation Based on Churn Risk

Customers were segmented into three risk groups based on churn probabilities:

| **Risk Group** | **Churn Probability Range** | **Count** |
| --- | --- | --- |
| **Low Risk** | `< 0.3` | 1,034 |
| **Medium Risk** | `0.3 - 0.7` | 228 |
| **High Risk** | `> 0.7` | 145 |

: Churn Probability and Risk Group Segmentation

```{r SVM-CustomerSegmentation, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Add churn probabilities and predicted class to the test data
testData$Churn_Prob <- probabilities[, "Yes"]
testData$Risk_Group <- cut(testData$Churn_Prob, 
                           breaks = c(0, 0.3, 0.7, 1),
                           labels = c("Low Risk", "Medium Risk", "High Risk"))

# View segmented data
head(testData[, c("Churn_Prob", "Risk_Group")])
```

**Insights**:

- **Low Risk**: Loyal customers with longer tenure and lower churn probability.
- **Medium Risk**: A transitional group that requires targeted retention efforts to prevent escalation to high risk.
- **High Risk**: Customers with the shortest tenure and highest churn probability, requiring immediate attention.

**Visualization**:

```{r SVM-VisualizingCustomerSegmentation, echo=FALSE, results='asis', message=FALSE, warning=FALSE, cache=TRUE}
# Plot churn probability by risk group
ggplot(testData, aes(x = Risk_Group, y = Churn_Prob)) +
  geom_boxplot(aes(fill = Risk_Group)) +
  ggtitle("Customer Segmentation by Churn Risk") +
  xlab("Risk Group") +
  ylab("Churn Probability") +
  theme_minimal()
```

A **boxplot** was used to visualize the distribution of churn probabilities across risk groups, highlighting distinct differences.


## Key Feature Insights

**Average Monthly Charges and Tenure by Risk Group**:

```{r SVM-FeatureImportance, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# ---Grouping Customers by Key Features---
# Summarize average monthly charges by risk group
segmentation_summary <- testData %>%
  group_by(Risk_Group) %>%
  summarise(
    Avg_MonthlyCharges = mean(MonthlyCharges),
    Avg_Tenure = mean(tenure),
    Count = n()
  )

print(segmentation_summary)

```

**Observations**:

- **High Risk** customers have the highest monthly charges and shortest tenure.
- **Low Risk** customers pay less on average but stay significantly longer.

**Actionable Steps**:

1. **High-Risk Customers**:
    - Offer discounts or loyalty incentives to reduce churn.
    - Focus on engagement strategies during the early months of the customer lifecycle.
2. **Medium-Risk Customers**:
    - Deploy retention campaigns, such as satisfaction surveys or targeted offers, to prevent escalation to high risk.
3. **Low-Risk Customers**:
    - Reward loyalty with perks, such as exclusive deals or access to premium services.
    
## Customer Retention Strategy

1. **Segment-Based Interventions**:
    - Customize strategies for each risk group.
2. **High-Risk Customers**:
    - Deploy immediate retention efforts, such as discounts or personalized outreach.
3. **Medium-Risk Customers**:
    - Monitor and engage proactively to prevent churn escalation.
4. **Low-Risk Customers**:
    - Continue engagement to maintain loyalty.

The SVM model provides robust predictions with high recall and precision, making it well-suited for customer churn analysis. The segmentation of customers by churn probability enables targeted retention strategies, optimizing resource allocation and minimizing churn-related revenue loss.


## Key Findings for TeleConnect Company

The analysis of various models provided critical insights into customer churn, enabling the identification of major churn predictors and actionable business strategies:


### Key Predictors of Customer Churn

1. **Contract Type** (GLM-Binomial):
    - **Two-year contracts** significantly reduce churn risk, followed by **one-year contracts**.
    - **Month-to-month contracts** have the highest churn rates and require attention.
2. **Internet Service**:
    - **Fiber optic users** have the highest churn risk, likely due to service reliability or cost concerns.
    - **DSL users** have moderate churn risk, while customers with no internet service show the lowest churn rates.
3. **Payment and Billing**:
    - **Electronic check users** exhibit a higher likelihood of churn.
    - **Paperless billing** is associated with increased churn.
    - **Automatic payments** correlate with better customer retention.


### Non-Linear Relationships and Behavioral Patterns (GAM)

1. **Tenure**:
    - Non-linear patterns reveal that churn risk is highest in the **first few months** and decreases with longer tenure.
    - Strong relationship captured through flexible modeling, indicating early-stage engagement is crucial.
2. **Total Charges**:
    - Non-linear effects suggest that customers with higher charges are at greater churn risk, requiring a more nuanced pricing strategy.
3. **Service Adoption**:
    - Customers using multiple services (e.g., online security, tech support) demonstrate higher retention rates.
    - Streaming services have minimal impact on churn behavior.

### Behavioral and Risk Insights

1. **Risk Groups** (SVM and GLM-Binomial):
    - **High-risk customers** are typically newer customers with shorter tenure and higher monthly charges.
    - **Medium-risk customers** show moderate churn probabilities and transitional behavior, requiring retention programs.
    - **Low-risk customers** are loyal, with longer tenure and lower churn probabilities.
2. **Feature Importance**:
    - **Contract type**, **internet service type**, and **payment method** are the most significant predictors across models.
3. **Segmentation**:
    - High-risk customers represent a **critical segment** for retention efforts.
    - Medium-risk customers provide an opportunity to proactively reduce churn before they escalate to high-risk.


### Model Contributions

1. **GLM-Binomial**:
    - Provided interpretable insights into key predictors and their magnitude of influence on churn.
    - Highlighted the importance of contract type, internet service, and payment methods.
2. **GAM**:
    - Captured non-linear patterns in tenure and total charges, offering a better understanding of churn behavior over time.
    - Enabled flexible modeling of relationships beyond linear assumptions.
3. **SVM**:
    - Effectively segmented customers into **risk groups** based on churn probabilities.
    - Provided actionable insights into high-risk customer profiles.
4. **Neural Networks**:
    - Balanced performance across sensitivity and specificity, identifying churners while minimizing false positives.

### Actionable Insights for TeleConnect

1. Focus on **retaining high-risk customers** (e.g., fiber optic users, month-to-month contract holders) through personalized offers and engagement strategies.
2. Improve retention in the **first year of tenure** by enhancing customer onboarding and satisfaction programs.
3. Encourage adoption of **automatic payments** and **long-term contracts** through loyalty rewards and discounts.
4. Promote **online security and tech support** services to improve customer retention.
5. Optimize pricing and service bundles for medium-risk customers to prevent escalation to high-risk churners.

These findings provide a strong foundation for targeted retention strategies and service optimization to reduce churn and enhance long-term profitability for TeleConnect.

# Conclusion

The analysis of customer churn for TeleConnect has provided actionable insights to address the company's churn challenges and improve customer retention. By leveraging multiple machine learning models, including GLM-Binomial, GAM, SVM, and Neural Networks, we identified critical factors influencing churn and developed a comprehensive understanding of customer behavior.

Key findings revealed that **contract type**, **internet service**, and **payment methods** are the most significant predictors of churn. Non-linear relationships in **tenure** and **total charges** highlighted the importance of engaging customers early and adopting flexible pricing strategies. Segmenting customers into risk groups using churn probabilities provided a clear framework for targeted retention efforts.

## Role of Generative AI

Generative AI significantly enhanced our understanding of machine learning models and streamlined project workflows. It simplified complex code, aided documentation, and provided clear visualizations, enabling deeper insights into model behavior. The AI also contributed to generating hypotheses, refining modeling techniques, and automating repetitive tasks, allowing the team to focus on strategic analysis and interpretation. This integration not only accelerated the project timeline but also improved our ability to document, understand, and communicate findings effectively.

## Final Thoughts

This project underscores the importance of data-driven decision-making in tackling customer churn. By combining advanced machine learning techniques with collaborative teamwork and generative AI tools, TeleConnect can implement targeted retention strategies and optimize customer satisfaction.

Moving forward, the integration of customer satisfaction metrics, competitive market data, and real-time prediction capabilities can enhance the accuracy and relevance of churn predictions. This continuous improvement process will ensure TeleConnect remains competitive while fostering long-term customer loyalty and profitability.

The success of this project highlights the transformative potential of combining human expertise, teamwork, and generative AI to drive innovation and achieve impactful business outcomes.

\newpage
\pagenumbering{gobble}

# Appendix


## References

\newpage

## Code Appendix

### Dataset Overview

```{r, echo=FALSE, results='show', cache=TRUE}
library(readr)

# Show the cleaned data as a table
d.cleaned_telecom_customer_churn <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

#classify variables types
variable_types <- sapply(d.cleaned_telecom_customer_churn, function(x) {
  if (is.numeric(x) && length(unique(x)) > 20) {
    "Continuous"
  } else if (length(unique(x)) == 2) {
    "Binomial"
  } else if (!is.numeric(x) || length(unique(x)) <= 20) {
    "Categorical"
  } else if (is.numeric(x) && all(x == as.integer(x))) {
    "Count"
  } else {
    "Other"
  }
})

print(variable_types)

#creating count datatypes:
# 1. Create service count variable (including all services)
d.cleaned_telecom_customer_churn$services_count <- rowSums(
  dplyr::select(d.cleaned_telecom_customer_churn,
                PhoneService, MultipleLines, InternetService, OnlineSecurity,
                OnlineBackup, DeviceProtection, TechSupport, StreamingTV,
                StreamingMovies) != "No"
)
```

### Linear Model

**Summary:**

```{r, echo=FALSE, results='show', message=FALSE, warning=FALSE, cache=TRUE}
# Load required libraries
library(ggplot2)

# Initial linear model with all relevant predictors
lm_initial <- lm(log(MonthlyCharges) ~ 
                   tenure + 
                   InternetService + 
                   Contract + 
                   StreamingTV + 
                   StreamingMovies +
                   OnlineSecurity +
                   OnlineBackup +
                   DeviceProtection +
                   TechSupport +
                   PhoneService,
                 data = d.cleaned_telecom_customer_churn)

# Get model summary
summary(lm_initial)
```


### Support Vector Machine

**Churn Probability Prediction:**

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache=TRUE}
# Load necessary libraries
library(e1071)
library(caret)
library(ggplot2)
library(pROC)
library(dplyr)

# Load the cleaned dataset
data <- read_csv(
  file.path(cleaned_data_dir, "telecom_customers_churn_cleaned.csv")
)

# Check for missing values
sum(is.na(data))

# Convert target variable to a factor
data$Churn <- as.factor(data$Churn)

# Split the dataset into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$Churn, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# ------------------ INITIAL SVM MODEL ------------------
# Train the SVM model
svm_model <- svm(Churn ~ ., data = trainData, kernel = "radial", cost = 1, 
                 gamma = 0.1)

# Make predictions on the test data
predictions <- predict(svm_model, testData)

# Evaluate the initial model
cat("\nConfusion Matrix for Initial SVM Model:\n")
confusionMatrix(predictions, testData$Churn)

# Paths to save tuned model and parameters
tuned_model_path <- "output/svm_tuned_model.rds"
tuned_params_path <- "output/svm_tuned_parameters.rds"

# ------------------ TUNED SVM MODEL ------------------
# Tune the SVM model for optimal hyperparameters or load if already saved
set.seed(123)
if (!file.exists(tuned_model_path) || !file.exists(tuned_params_path)) {
  cat("\nTuning SVM Model...\n")
  tuned_parameters <- tune(svm, Churn ~ ., data = trainData,
                           ranges = list(cost = c(0.1, 1, 10), 
                                         gamma = c(0.01, 0.1, 1)),
                           probability = TRUE)
  best_model <- tuned_parameters$best.model
  saveRDS(best_model, tuned_model_path)
  saveRDS(tuned_parameters, tuned_params_path)
} else {
  cat("\nLoading Tuned SVM Model...\n")
  best_model <- readRDS(tuned_model_path)
  tuned_parameters <- readRDS(tuned_params_path)
}

# Make predictions with the best model
best_predictions <- predict(best_model, testData, probability = TRUE)

# Evaluate the best model
cat("\nConfusion Matrix for Tuned SVM Model:\n")
conf_matrix <- confusionMatrix(best_predictions, testData$Churn)
print(conf_matrix)

# Extract probabilities
probabilities <- attr(predict(best_model, testData, probability = TRUE), "probabilities")

# ------------------ BEST PARAMETERS ------------------
# Check the best parameters chosen during tuning
cat("\nBest Parameters Chosen During Tuning:\n")
print(tuned_parameters$best.parameters)

# ------------------ PROBABILITIES ------------------
# Extract probabilities
probabilities <- attr(best_predictions, "probabilities")
cat("\nSample of Predicted Probabilities:\n")
print(head(probabilities))  # Display a sample of probabilities
```
